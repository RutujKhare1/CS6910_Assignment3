# -*- coding: utf-8 -*-
"""fdl-a3-1(5).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e8UQaY4xCzUixYNpcx1AcGuUqWw2eZ9C
"""


import matplotlib.pyplot as plt
import matplotlib.image as img
import numpy as np
import pandas as pd
import torch
import random
import wandb
import torch.nn as nn
import csv

if torch.cuda.is_available():
    device_name = torch.device("cuda")
else:
    device_name = torch.device('cpu')
print("Using {}.".format(device_name))

def preprocessingData(df, max_len, eng_to_int, hin_to_int):
    eng_words = df['eng'].copy()
    for i in range(len(eng_words)):
        l = len(eng_words[i])
        eng_words[i] = eng_words[i] + "*"*(max_len - l + 3)
    hin_words = df['hin'].copy()
    for i in range(len(hin_words)):
        l = len(hin_words[i])
        hin_words[i] = "#" + hin_words[i] + "*"*(max_len - l + 2)

    index_eng_words = []
    for eng_word in eng_words:
        index_eng_word = [eng_to_int[i] for i in eng_word]
        index_eng_words.append(index_eng_word)
    index_hin_words = []
    for hin_word in hin_words:
        index_hin_word = [hin_to_int[i] if i in hin_to_int else hin_to_int['_'] for i in hin_word]
        index_hin_words.append(index_hin_word)
    tensor_eng = torch.tensor(index_eng_words).to(device_name)
    tensor_hin = torch.tensor(index_hin_words).to(device_name)
    return tensor_eng, tensor_hin

def calculateAccuracy(trained_pred, y_true):
    out = []
    ten_pred = torch.tensor(trained_pred)
    for i in range(len(trained_pred)):
        temp = ten_pred[i].T
        out.extend(temp)
    y_pred = torch.stack(out).to(device_name)
    cnt = 0
    for i,j in zip(y_pred, y_true):
        cor = torch.eq(i, j)
        if(torch.mean(cor.float()).item() == 1.0):
            cnt += 1
    return cnt / len(y_pred)

class GRU_Encoder(nn.Module):
    def __init__(self, input_size, hid_size, num_of_enc_layers, emb_size, batch_size, dropout, bi_direct):
        super(GRU_Encoder, self).__init__()
        self.input_size = input_size
        self.hid_size = hid_size
        self.num_of_enc_layers = num_of_enc_layers
        self.emb_size = emb_size
        self.batch_size = batch_size
        self.bi_direct = bi_direct
        self.dropout = dropout
        self.embedding = nn.Embedding(input_size, emb_size)
        # print("IS:{} ES:{}".format(input_size, emb_size))
        self.gru = nn.GRU(emb_size, hid_size, num_of_enc_layers, bidirectional = bi_direct, dropout = dropout)

    def forward(self, input_data, hidden):
        input_data = input_data.T
        # print(input_data.shape)
        embed = self.embedding(input_data).to(device_name)
        output, hidden = self.gru(embed, hidden)
        return output, hidden

    def initialiseHidden(self):
        if(self.bi_direct):
            return torch.zeros(2*self.num_of_enc_layers, self.batch_size, self.hid_size, device = device_name)
        else:
            return torch.zeros(self.num_of_enc_layers, self.batch_size, self.hid_size, device = device_name)

class GRU_Decoder(nn.Module):
    def __init__(self, op_size, num_of_dec_layers, hid_size, batch_size, emb_size, dropout, bi_direct):
        super(GRU_Decoder, self).__init__()
        self.op_size = op_size
        self.hid_size = hid_size
        self.num_of_dec_layers = num_of_dec_layers
        self.emb_size = emb_size
        self.batch_size = batch_size
        self.bi_direct = bi_direct
        self.embedding = nn.Embedding(op_size, emb_size)
        self.op = nn.Linear(2*hid_size, op_size) if (bi_direct) else nn.Linear(hid_size, op_size)
        self.softmax = nn.LogSoftmax(dim = 2)
        self.gru = nn.GRU(emb_size, hid_size, num_of_dec_layers, bidirectional = bi_direct, dropout = dropout)

    def forward(self, input_data, hidden):
        # print(input_data)
        embed = self.embedding(input_data)
        embed = embed.view(-1, self.batch_size, self.emb_size)
        #     print(hidden.shape)
        out, hidden = self.gru(embed, hidden)
        # print(out.shape)
        temp = self.op(out)
        out = self.softmax(temp)
        return out, hidden

class RNN_Encoder(nn.Module):
    def __init__(self, input_size, hid_size, num_of_enc_layers, emb_size, batch_size, dropout, bi_direct):
        super(RNN_Encoder, self).__init__()
        self.input_size = input_size
        self.hid_size = hid_size
        self.num_of_enc_layers = num_of_enc_layers
        self.emb_size = emb_size
        self.batch_size = batch_size
        self.bi_direct = bi_direct
        self.dropout = dropout
        self.embedding = nn.Embedding(input_size, emb_size)
        self.rnn = nn.RNN(emb_size, hid_size, num_of_enc_layers, bidirectional = bi_direct, dropout = dropout)

    def forward(self, input_data, hidden):
        input_data = input_data.T
        embed = self.embedding(input_data).to(device_name)
        output, hidden = self.rnn(embed, hidden)
        return output, hidden

    def initialiseHidden(self):
        if(self.bi_direct):
            return torch.zeros(2*self.num_of_enc_layers, self.batch_size, self.hid_size, device = device_name)
        else:
            return torch.zeros(self.num_of_enc_layers, self.batch_size, self.hid_size, device = device_name)

class RNN_Decoder(nn.Module):
    def __init__(self, op_size, num_of_dec_layers, hid_size, batch_size, emb_size, dropout, bi_direct):
        super(RNN_Decoder, self).__init__()
        self.op_size = op_size
        self.hid_size = hid_size
        self.num_of_dec_layers = num_of_dec_layers
        self.emb_size = emb_size
        self.batch_size = batch_size
        self.bi_direct = bi_direct
        self.embedding = nn.Embedding(op_size, emb_size)
        self.op = nn.Linear(2*hid_size, op_size) if (bi_direct) else nn.Linear(hid_size, op_size)
        self.softmax = nn.LogSoftmax(dim = 2)
        self.rnn = nn.RNN(emb_size, hid_size, num_of_dec_layers, bidirectional = bi_direct, dropout = dropout)

    def forward(self, input_data, hidden):
        embed = self.embedding(input_data)
        embed = embed.view(-1, self.batch_size, self.emb_size)
        out, hidden = self.rnn(embed, hidden)
        temp = self.op(out)
        out = self.softmax(temp)
        return out, hidden

class LSTM_Encoder(nn.Module):
    def __init__(self, input_size, hid_size, num_of_enc_layers, emb_size, batch_size, dropout, bi_direct):
        super(LSTM_Encoder, self).__init__()
        self.input_size = input_size
        self.hid_size = hid_size
        self.num_of_enc_layers = num_of_enc_layers
        self.emb_size = emb_size
        self.batch_size = batch_size
        self.bi_direct = bi_direct
        self.dropout = dropout
        self.embedding = nn.Embedding(input_size, emb_size)
        self.lstm = nn.LSTM(emb_size, hid_size, num_of_enc_layers, bidirectional = bi_direct, dropout = dropout)

    def forward(self, input_data, hidden, state):
        input_data = input_data.T
        embed = self.embedding(input_data).to(device_name)
        output, (hidden, state) = self.lstm(embed, (hidden, state))
        return output, hidden, state

    def initialiseHidden(self):
        if(self.bi_direct):
            return torch.zeros(2*self.num_of_enc_layers, self.batch_size, self.hid_size, device = device_name)
        else:
            return torch.zeros(self.num_of_enc_layers, self.batch_size, self.hid_size, device = device_name)

class LSTM_Decoder(nn.Module):
    def __init__(self, op_size, num_of_dec_layers, hid_size, batch_size, emb_size, dropout, bi_direct):
        super(LSTM_Decoder, self).__init__()
        self.op_size = op_size
        self.hid_size = hid_size
        self.num_of_dec_layers = num_of_dec_layers
        self.emb_size = emb_size
        self.batch_size = batch_size
        self.bi_direct = bi_direct
        self.embedding = nn.Embedding(op_size, emb_size)
        self.op = nn.Linear(2*hid_size, op_size) if (bi_direct) else nn.Linear(hid_size, op_size)
        self.softmax = nn.LogSoftmax(dim = 2)
        self.lstm = nn.LSTM(emb_size, hid_size, num_of_dec_layers, bidirectional = bi_direct, dropout = dropout)

    def forward(self, input_data, hidden, state):
        embed = self.embedding(input_data)
        embed = embed.view(-1, self.batch_size, self.emb_size)
        out, (hidden, state) = self.lstm(embed, (hidden, state))
        temp = self.op(out)
        out = self.softmax(temp)
        return out, hidden, state

class Atten_decoder(nn.Module):
    def __init__(self, input_size, embedding_size, hidden_size, output_size, dec_layers, p, max_input_size, cell_type, bidirectional):
        super(Atten_decoder, self).__init__()
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.max_length =   max_input_size 
        self.dec_layers = dec_layers
        self.dropout = nn.Dropout(p)
        self.cell_type = cell_type
        self.softmax = nn.LogSoftmax(dim = 0)
        self.embedding = nn.Embedding(output_size, embedding_size)
        if(cell_type == "GRU"):
            self.gru = nn.GRU(hidden_size, hidden_size, dec_layers, dropout = p)
        if(cell_type == "RNN"):
            self.rnn = nn.RNN(hidden_size, hidden_size, dec_layers, dropout = p)
        if(cell_type == "LSTM"):
            self.lstm = nn.LSTM(hidden_size, hidden_size, dec_layers, dropout = p)
        self.fc = nn.Linear(hidden_size, output_size)  # fully connected.
        self.attn = nn.Linear(hidden_size+embedding_size, self.max_length)
        if(bidirectional):
            self.attn_combine = nn.Linear(hidden_size * 2 + embedding_size, hidden_size)
        else :
            self.attn_combine = nn.Linear(hidden_size + embedding_size, hidden_size)

    def forward(self, x,output, hidden, cell = 0):
        x = x.unsqueeze(0)
        output=output.permute(1,0,2)
#         print("X :", x.shape)
        embedded = self.embedding(x)
        embedded = self.dropout(embedded)
#         print("Emb-{},\nHid-{}".format(embedded.shape, hidden.shape))
        attn_weights = self.softmax(self.attn(torch.cat((embedded[0],hidden[0]), 2)))
        attn_applied = torch.bmm(attn_weights.unsqueeze(1),output)
        attn_applied = attn_applied.squeeze(1)
        op = torch.cat((embedded[0], attn_applied), 1)

        op = self.attn_combine(op).unsqueeze(0)
        op = nn.functional.relu(op)
        if(self.cell_type == "GRU"):
            outputs, hidden = self.gru(op, hidden)
        if(self.cell_type == "RNN"):
            outputs, hidden = self.rnn(op, hidden)
        if(self.cell_type == "LSTM"):
            outputs, (hidden, cell) = self.lstm(op, (hidden, cell))
        predictions = self.fc(outputs)
        # shape of predictions: (1, N, length_of_vocab)
        predictions = predictions.squeeze(0)
        # shape of predictions: (N, length_of_vocab)
        if(self.cell_type == "LSTM"):
            return predictions, hidden, attn_weights, attn_applied, cell
        return predictions, hidden ,attn_weights, attn_applied

def trainWithoutAttention(input_data, target_data, loss_fn, enc_optimizer, dec_optimizer, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type):
    teacher_forcing = 0.5
    loss = 0
    for b in range(0, len(input_data), batch_size):
        x, y = input_data[b : b+batch_size], target_data[b : b+batch_size]
        temp = 0
        enc_optimizer.zero_grad()
        dec_optimizer.zero_grad()
        if(cell_type == 'GRU' or cell_type == 'RNN'):
            enc_hidden = encoder.initialiseHidden()
            enc_output, enc_hidden = encoder(x, enc_hidden)
            dec_hidden = enc_hidden[-1].repeat(num_of_dec_layers, 1, 1)
            if bi_direct:
                dec_hidden = dec_hidden.repeat(2,1,1)
            y = y.T
            dec_input = y[0]
            #       print("AFT_Decoder Hidden : {}".format(dec_hidden.shape))
            condition = False if random.random() > teacher_forcing else True
            if(condition):
                for i in range(len(y)):
                    dec_output, dec_hidden = decoder(dec_input, dec_hidden)
                    temp += loss_fn(torch.squeeze(dec_output), y[i])
                    dec_input = y[i]
            else:
                for i in range(len(y)):
                    dec_output, dec_hidden = decoder(dec_input, dec_hidden)
                    prob, idx = dec_output.topk(1)
                    temp += loss_fn(torch.squeeze(dec_output), y[i])
                    dec_input = idx.squeeze().detach()
                    
        elif(cell_type == 'LSTM'):
            enc_hidden = encoder.initialiseHidden()
            enc_state = encoder.initialiseHidden()
            
            enc_output, enc_hidden, enc_state = encoder(x, enc_hidden, enc_state)
            dec_hidden = enc_hidden[-1].repeat(num_of_dec_layers, 1, 1)
            if bi_direct:
                dec_hidden = dec_hidden.repeat(2,1,1)
            
            dec_state = enc_state[-1].repeat(num_of_dec_layers, 1, 1)
            if bi_direct:
                dec_state = dec_state.repeat(2,1,1)
            y = y.T
            dec_input = y[0]
            #       print("AFT_Decoder Hidden : {}".format(dec_hidden.shape))
            condition = False if random.random() > teacher_forcing else True
            if(condition):
                for i in range(len(y)):
                    dec_output, dec_hidden, dec_state = decoder(dec_input, dec_hidden, dec_state)
                    temp += loss_fn(torch.squeeze(dec_output), y[i])
                    dec_input = y[i]
            else:
                for i in range(len(y)):
                    dec_output, dec_hidden, dec_state = decoder(dec_input, dec_hidden, dec_state)
                    prob, idx = dec_output.topk(1)
                    temp += loss_fn(torch.squeeze(dec_output), y[i])
                    dec_input = idx.squeeze().detach()
        
        temp.backward()
        enc_optimizer.step()
        dec_optimizer.step()
        loss += temp

    return loss.item()/(len(target_data) * target_data.shape[1]), encoder, decoder

def evalWithoutAttention(input_data, target_data, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type):
    out = []
    for b in range(0, len(input_data), batch_size):
        x, y = input_data[b : b+batch_size], target_data[b : b+batch_size]
        encoder.eval()
        decoder.eval()
        predicted_data = list()
        if(cell_type == 'GRU' or cell_type == 'RNN'):
            enc_hidden = encoder.initialiseHidden()
            enc_output, enc_hidden = encoder(x, enc_hidden)
            y = y.T      
            dec_hidden = enc_hidden[-1].repeat(num_of_dec_layers, 1, 1)
            #dec_hidden = enc_hidden
            dec_input = y[0]
            if bi_direct:
                dec_hidden = dec_hidden.repeat(2,1,1)
            for i in range(len(y)):
                dec_output, dec_hidden = decoder(dec_input, dec_hidden)
                prob, idx = dec_output.topk(1)
                idx = idx.squeeze()
                dec_input = idx
                predicted_data.append(idx.tolist())
            out.append(predicted_data)
        elif(cell_type == 'LSTM'):
            enc_hidden = encoder.initialiseHidden()
            enc_state = encoder.initialiseHidden()
            enc_output, enc_hidden, enc_state = encoder(x, enc_hidden, enc_state)
            y = y.T      
            dec_hidden = enc_hidden[-1].repeat(num_of_dec_layers, 1, 1)
            if bi_direct:
                dec_hidden = dec_hidden.repeat(2,1,1)
                
            dec_state = enc_state[-1].repeat(num_of_dec_layers, 1, 1)
            if bi_direct:
                dec_state = dec_state.repeat(2,1,1)
            
            dec_input = y[0]
            for i in range(len(y)):
                dec_output, dec_hidden, dec_state = decoder(dec_input, dec_hidden, dec_state)
                prob, idx = dec_output.topk(1)
                idx = idx.squeeze()
                dec_input = idx
                predicted_data.append(idx.tolist())
            out.append(predicted_data)
    return out

def trainWithAttention(input_data, target_data, loss_fn, enc_optimizer, dec_optimizer, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type):
    teacher_forcing = 0.5
    loss = 0
    for b in range(0, len(input_data), batch_size):
        x, y = input_data[b : b+batch_size], target_data[b : b+batch_size]
        temp = 0
        enc_optimizer.zero_grad()
        dec_optimizer.zero_grad()
        if(cell_type == 'GRU' or cell_type == 'RNN'):
            enc_hidden = encoder.initialiseHidden()
            enc_output, enc_hidden = encoder(x, enc_hidden)
            dec_hidden = enc_hidden[-1].repeat(num_of_dec_layers, 1, 1)
            if bi_direct:
                dec_hidden = dec_hidden.repeat(2,1,1)
            y = y.T
            dec_input = y[0]
            #       print("AFT_Decoder Hidden : {}".format(dec_hidden.shape))
            condition = False if random.random() > teacher_forcing else True
            if(condition):
                for i in range(len(y)):
                    dec_output, dec_hidden , attn_weights, attn_applied= decoder(dec_input, enc_output, dec_hidden)
                    temp += loss_fn(torch.squeeze(dec_output), y[i])
                    dec_input = y[i]
            else:
                for i in range(len(y)):
                    dec_output, dec_hidden , attn_weights, attn_applied= decoder(dec_input, enc_output, dec_hidden)
                    prob, idx = dec_output.topk(1)
                    temp += nn.NLLLoss()(nn.functional.log_softmax(dec_output, dim=1), y[i])
#                     temp += loss_fn(torch.squeeze(dec_output), y[i])
                    dec_input = idx.squeeze().detach()
                    
        elif(cell_type == 'LSTM'):
            enc_hidden = encoder.initialiseHidden()
            enc_state = encoder.initialiseHidden()
            
            enc_output, enc_hidden, enc_state = encoder(x, enc_hidden, enc_state)
            dec_hidden = enc_hidden[-1].repeat(num_of_dec_layers, 1, 1)
            if bi_direct:
                dec_hidden = dec_hidden.repeat(2,1,1)
            
            dec_state = enc_state[-1].repeat(num_of_dec_layers, 1, 1)
            if bi_direct:
                dec_state = dec_state.repeat(2,1,1)
            y = y.T
            dec_input = y[0]
            #       print("AFT_Decoder Hidden : {}".format(dec_hidden.shape))
            condition = False if random.random() > teacher_forcing else True
            if(condition):
                for i in range(len(y)):
                    dec_output, dec_hidden, attn_weights, attn_applied, dec_state = decoder(dec_input, enc_output, dec_hidden, dec_state)
                    temp += loss_fn(torch.squeeze(dec_output), y[i])
                    dec_input = y[i]
            else:
                for i in range(len(y)):
                    dec_output, dec_hidden, attn_weights, attn_applied, dec_state = decoder(dec_input, enc_output, dec_hidden, dec_state)
                    prob, idx = dec_output.topk(1)
#                     print("blhhh")
                    temp += nn.NLLLoss()(nn.functional.log_softmax(dec_output, dim=1), y[i])
#                     temp += loss_fn(torch.squeeze(dec_output), y[i])
                    dec_input = idx.squeeze().detach()
        
        temp.backward()
        enc_optimizer.step()
        dec_optimizer.step()
        loss += temp

    return loss.item()/(len(target_data) * target_data.shape[1]), encoder, decoder, attn_weights, attn_applied

def evalWithAttention(input_data, target_data, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type):
    out = []
    for b in range(0, len(input_data), batch_size):
        x, y = input_data[b : b+batch_size], target_data[b : b+batch_size]
        encoder.eval()
        decoder.eval()
        predicted_data = list()
        if(cell_type == 'GRU' or cell_type == 'RNN'):
            enc_hidden = encoder.initialiseHidden()
            enc_output, enc_hidden = encoder(x, enc_hidden)
            y = y.T      
            dec_hidden = enc_hidden[-1].repeat(num_of_dec_layers, 1, 1)
            #dec_hidden = enc_hidden
            dec_input = y[0]
            if bi_direct:
                dec_hidden = dec_hidden.repeat(2,1,1)
            for i in range(len(y)):
#                 dec_output, dec_hidden = decoder(dec_input, dec_hidden)
                dec_output, dec_hidden , attn_weights, attn_applied= decoder(dec_input, enc_output, dec_hidden)
                prob, idx = dec_output.topk(1)
                idx = idx.squeeze()
                dec_input = idx
                predicted_data.append(idx.tolist())
            out.append(predicted_data)
        elif(cell_type == 'LSTM'):
            enc_hidden = encoder.initialiseHidden()
            enc_state = encoder.initialiseHidden()
            enc_output, enc_hidden, enc_state = encoder(x, enc_hidden, enc_state)
            y = y.T      
            dec_hidden = enc_hidden[-1].repeat(num_of_dec_layers, 1, 1)
            if bi_direct:
                dec_hidden = dec_hidden.repeat(2,1,1)
                
            dec_state = enc_state[-1].repeat(num_of_dec_layers, 1, 1)
            if bi_direct:
                dec_state = dec_state.repeat(2,1,1)
            
            dec_input = y[0]
            for i in range(len(y)):
                dec_output, dec_hidden, attn_weights, attn_applied, dec_state = decoder(dec_input, enc_output, dec_hidden, dec_state)
                prob, idx = dec_output.topk(1)
                idx = idx.squeeze()
                dec_input = idx
                predicted_data.append(idx.tolist())
            out.append(predicted_data)
    return out

def training(input_data, input_size, target_data, target_size, max_input_size, epochs, batch_size, emb_size, num_of_enc_layers, num_of_dec_layers, hid_size, cell_type, bi_direct, enc_dropout, dec_dropout, use_attention, beam_size):
    learning_rate = 0.001
    if(use_attention):
        if(cell_type == "GRU"):
            encoder = GRU_Encoder(input_size, hid_size, num_of_enc_layers, emb_size, batch_size, enc_dropout, bi_direct).to(device_name)
        elif(cell_type == "RNN"):
            encoder = RNN_Encoder(input_size, hid_size, num_of_enc_layers, emb_size, batch_size, enc_dropout, bi_direct).to(device_name)
        elif(cell_type == "LSTM"):
            encoder = LSTM_Encoder(input_size, hid_size, num_of_enc_layers, emb_size, batch_size, enc_dropout, bi_direct).to(device_name)
        decoder = Atten_decoder(input_size, emb_size, hid_size, target_size, num_of_dec_layers, dec_dropout, max_input_size, cell_type, bi_direct).to(device_name)
    else:
        if(cell_type == "GRU"):
            encoder = GRU_Encoder(input_size, hid_size, num_of_enc_layers, emb_size, batch_size, enc_dropout, bi_direct).to(device_name)
            decoder = GRU_Decoder(target_size, num_of_dec_layers, hid_size, batch_size, emb_size, dec_dropout, bi_direct).to(device_name)
        elif(cell_type == "RNN"):
            encoder = RNN_Encoder(input_size, hid_size, num_of_enc_layers, emb_size, batch_size, enc_dropout, bi_direct).to(device_name)
            decoder = RNN_Decoder(target_size, num_of_dec_layers, hid_size, batch_size, emb_size, dec_dropout, bi_direct).to(device_name)
        elif(cell_type == "LSTM"):
            encoder = LSTM_Encoder(input_size, hid_size, num_of_enc_layers, emb_size, batch_size, enc_dropout, bi_direct).to(device_name)
            decoder = LSTM_Decoder(target_size, num_of_dec_layers, hid_size, batch_size, emb_size, dec_dropout, bi_direct).to(device_name)

    enc_optimizer = torch.optim.Adam(encoder.parameters(), learning_rate)
    dec_optimizer = torch.optim.Adam(decoder.parameters(), learning_rate)
    loss_fn = nn.NLLLoss(reduction = 'sum')
    encoder.train()
    decoder.train()
    train_loss = []
    train_acc = []
    val_acc = []
    temp1 = 0
    temp2 = 0
    if(use_attention):
        for i in range(epochs):
            encoder.train()
            decoder.train()
            loss, encoder, decoder ,attn_weights, attn_applied = trainWithAttention(input_data, target_data, loss_fn, enc_optimizer, dec_optimizer, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type)
            train_loss.append(loss)
            trained_pred = evalWithAttention(tensor_eng, tensor_hin, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type)
            acc = calculateAccuracy(trained_pred, tensor_hin)
            train_acc.append(acc)
            trained_pred_val = evalWithAttention(tensor_eng_val, tensor_hin_val, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type)
            vacc = calculateAccuracy(trained_pred_val, tensor_hin_val)
            val_acc.append(vacc)
            print("Epoch : {} \tLoss : {}".format(i, loss))
        
        temp1 = attn_weights
        temp2 = attn_applied
    else:
        for i in range(epochs):
            encoder.train()
            decoder.train()
            loss, encoder, decoder = trainWithoutAttention(input_data, target_data, loss_fn, enc_optimizer, dec_optimizer, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type)
            train_loss.append(loss)
            trained_pred = evalWithoutAttention(tensor_eng, tensor_hin, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type)
            acc = calculateAccuracy(trained_pred, tensor_hin)
            train_acc.append(acc)
            trained_pred_val = evalWithoutAttention(tensor_eng_val, tensor_hin_val, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type)
            vacc = calculateAccuracy(trained_pred_val, tensor_hin_val)
            val_acc.append(vacc)
            print("Epoch : {} \tLoss : {}".format(i, loss))
    
    return encoder, decoder, train_loss, train_acc, val_acc, temp1, temp2

#Loading the dataset and preprocessing

df = pd.read_csv('/content/hin_train.csv', names=['eng','hin'])
df_test = pd.read_csv('/content/hin_test.csv', names=['eng','hin'])
df_valid = pd.read_csv('/content/hin_valid.csv', names=['eng','hin'])
eng_maxlen = len(max(df['eng'], key=len))
hin_maxlen = len(max(df['hin'], key=len))
max_len = max(eng_maxlen, hin_maxlen)
eng_words = df['eng'].copy()
hin_words = df['hin'].copy()

unique_eng_letters = set(''.join(eng_words))
unique_eng_letters.add('*')


unique_hin_letters = set(''.join(hin_words))
unique_hin_letters.add('#')
unique_hin_letters.add('*')

int_to_eng = dict(enumerate(unique_eng_letters))
eng_to_int = {char: ind for ind, char in int_to_eng.items()}

int_to_hin = dict(enumerate(unique_hin_letters))
hin_to_int = {char: ind for ind, char in int_to_hin.items()}
hin_to_int['_'] = len(hin_to_int)

tensor_eng, tensor_hin = preprocessingData(df, max_len, eng_to_int, hin_to_int)
tensor_eng_test, tensor_hin_test = preprocessingData(df_test, max_len, eng_to_int, hin_to_int)
tensor_eng_val, tensor_hin_val = preprocessingData(df_valid, max_len, eng_to_int, hin_to_int)

#Configuration of the model

input_data = tensor_eng
input_size = len(unique_eng_letters)
target_data = tensor_hin
target_size = len(unique_hin_letters) 
max_input_size = tensor_eng.shape[1] 
epochs = 10
batch_size = 256 
emb_size = 512 
num_of_enc_layers = 3
num_of_dec_layers = 3
hid_size = 512
cell_type = "LSTM" 
bi_direct = True 
enc_dropout = 0.3
dec_dropout = 0.1
beam_size = 1
use_attention = False

#Block of code to train the model

encoder, decoder, train_loss, train_acc, val_acc, temp1, temp2 = training(input_data, input_size, target_data, target_size, max_input_size, epochs, batch_size, emb_size, num_of_enc_layers, num_of_dec_layers, hid_size, cell_type, bi_direct, enc_dropout, dec_dropout, use_attention, beam_size)
if(use_attention):
    trained_pred = evalWithAttention(tensor_eng, tensor_hin, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type)
else:
    trained_pred = evalWithoutAttention(tensor_eng, tensor_hin, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type)
print(calculateAccuracy(trained_pred, tensor_hin))

#BLock of code to see the test accuracy

if(use_attention):
    trained_pred = evalWithAttention(tensor_eng_test, tensor_hin_test, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type)
else:
    trained_pred = evalWithoutAttention(tensor_eng_test, tensor_hin_test, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type)
print(calculateAccuracy(trained_pred, tensor_hin_test))