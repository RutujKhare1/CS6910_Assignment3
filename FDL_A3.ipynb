{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1hWD2qa_DQt8gAoz15e9AibGeIQsuwXH_",
      "authorship_tag": "ABX9TyP4JHJUIiYhiEK8Oga8TA8c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RutujKhare1/CS6910_Assignment3/blob/main/FDL_A3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "0nunHnybCpgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "P0EEiw_RlOn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U2XJa6S3Yk4B"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/FDL_A3/hin/hin_train.csv', names=['eng','hin'])"
      ],
      "metadata": {
        "id": "6srSwMqgClUB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_eng_letters = set(''.join(df['eng']))\n",
        "unique_hin_letters = set(''.join(df['hin']))\n",
        "int_to_eng = dict(enumerate(unique_eng_letters))\n",
        "eng_to_int = {char: ind+1 for ind, char in int_to_eng.items()}\n",
        "\n",
        "int_to_hin = dict(enumerate(unique_hin_letters))\n",
        "hin_to_int = {char: ind+1 for ind, char in int_to_hin.items()}"
      ],
      "metadata": {
        "id": "-QBfA5AnqbdH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_words = df['eng']\n",
        "maxlen = len(max(df['eng'], key=len))\n",
        "index_eng_words = []\n",
        "for eng_word in eng_words:\n",
        "  index_eng_word = [eng_to_int[i] for i in eng_word]\n",
        "  l = len(index_eng_word)\n",
        "  index_eng_word.extend([0]*(maxlen-l))\n",
        "  index_eng_words.append(index_eng_word)"
      ],
      "metadata": {
        "id": "bIXxp34WqlzO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hin_words = df['hin']\n",
        "maxlen = len(max(df['hin'], key=len))\n",
        "index_hin_words = []\n",
        "for hin_word in hin_words:\n",
        "  index_hin_word = [hin_to_int[i] for i in hin_word]\n",
        "  l = len(index_hin_word)\n",
        "  index_hin_word.extend([0]*(maxlen-l))\n",
        "  index_hin_words.append(index_hin_word)"
      ],
      "metadata": {
        "id": "vBUP1K6jolVb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_eng = torch.tensor(index_eng_words)\n",
        "tensor_hin = torch.tensor(index_hin_words)"
      ],
      "metadata": {
        "id": "BaHcaexBFjO2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_eng"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yl797SncFjGp",
        "outputId": "8cbba5ea-a73a-4b06-8846-3a6a884f0c5d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[15, 10,  5,  ...,  0,  0,  0],\n",
              "        [ 6, 14, 25,  ...,  0,  0,  0],\n",
              "        [24, 14, 23,  ...,  0,  0,  0],\n",
              "        ...,\n",
              "        [ 5, 15,  5,  ...,  0,  0,  0],\n",
              "        [15, 12,  2,  ...,  0,  0,  0],\n",
              "        [ 5, 25,  8,  ...,  0,  0,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GRU_Encoder(nn.Module):\n",
        "  def __init__(self, input_size, hid_size, num_of_enc_layers, emb_size, batch_size, bi_direct):\n",
        "    self.input_size = input_size\n",
        "    self.hid_size = hid_size\n",
        "    self.emb_size = emb_size\n",
        "    self.batch_size = batch_size\n",
        "    self.bi_direct = bi_direct\n",
        "    self.embedding = nn.Embedding(input_size, emb_size)\n",
        "    self.gru = nn.GRU(emb_size, hid_size, num_of_enc_layers, bidirectional = bi_direct)\n",
        "\n",
        "  def forwardPropagation(self, input_data, hidden):\n",
        "    embed = self.embedding(input_data).view(-1, self.batch_size, self.hid_size)\n",
        "    output, hidden = self.gru(embed, hidden)\n",
        "    if(self.bi_direct):\n",
        "      hidden = hidden.resize(2, self.num_of_enc_layers, self.batch_size, self.hid_size)\n",
        "      hidden = torch.add(hidden[0], hidden[1])/2\n",
        "    return output, hidden\n",
        "  \n",
        "  def initialiseHidden(self):\n",
        "    if(self.bi_direct):\n",
        "      return torch.zeros(2*self.num_of_enc_layers, self.batch_size, self.hid_size)\n",
        "    else:\n",
        "      return torch.zeros(self.num_of_enc_layers, self.batch_size, self.hid_size)\n",
        "  "
      ],
      "metadata": {
        "id": "S4q529z-FjCx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRU_Decoder(nn.Module):\n",
        "  def __init__(self, op_size, num_of_dec_layers, hid_size, batch_size, emb_size):\n",
        "    self.op_size = op_size\n",
        "    self.hid_size = hid_size\n",
        "    self.num_of_dec_layers = num_of_dec_layers\n",
        "    self.emb_size = emb_size\n",
        "    self.batch_size = batch_size\n",
        "    self.embedding = nn.Embedding(op_size, emb_size)\n",
        "    self.op = nn.Linear(hid_size, op_size)\n",
        "    self.softmax = nn.LogSoftmax(dim = 1)\n",
        "\n",
        "  def forwardPropagation(self, input_data, hidden):\n",
        "    embed = self.embedding(input_data).view(-1, self.batch_size, self.emb_size)\n",
        "    out, hidden = self.gru(embed, hidden)\n",
        "    out = self.softmax(self.op(out))\n",
        "    return out, hidden"
      ],
      "metadata": {
        "id": "uDr5Mjb6Fi0a"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(input_data, loss_fn, enc_optimizer, dec_optimizer, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, hid_size, cell_type):\n",
        "  loss = 0\n",
        "  teacher_forcing = 0.5\n",
        "  for x, y in input_data:\n",
        "    temp = 0\n",
        "    enc_optimizer.zero_grad()\n",
        "    dec_optimizer.zero_grad()\n",
        "    # x = x.T\n",
        "    # y = y.T\n",
        "    t_step = len(x)\n",
        "    if(cell_type == 'GRU'):\n",
        "      enc_hidden = encoder.initialiseHidden()\n",
        "      enc_output, enc_hidden = encoder(x, enc_hidden)\n",
        "      \n",
        "      if(num_of_dec_layers > num_of_enc_layers):\n",
        "        num = num_of_dec_layers\n",
        "        dec_hidden = enc_hidden\n",
        "        while(num != num_of_enc_layers):\n",
        "          dec_hidden = torch.cat([dec_hidden, enc_hidden[-1].unsqueeze(0)], dim = 0)\n",
        "          num -= 1\n",
        "      elif(num_of_dec_layers < num_of_enc_layers):\n",
        "        dec_hidden = enc_hidden[-num_of_dec_layers:]\n",
        "      else:\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "      dec_input = y[0]\n",
        "      \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KToV_mC2WtPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training(input_data, input_size, target_size, max_input_size, epochs, batch_size, emb_size, num_of_enc_layers, num_of_dec_layers, hid_size, cell_type, bi_direct, dropout, beam_size):\n",
        "  learning_rate = 0.01\n",
        "  if(cell_type == \"GRU\"):\n",
        "    encoder = GRU_Encoder(input_size, hid_size, num_of_enc_layers, emb_size, batch_size, bi_direct)\n",
        "    decoder = GRU_Decoder(target_size, num_of_dec_layers, hid_size, batch_size, emb_size)\n",
        "  \n",
        "  enc_optimizer = torch.optim.Adam(encoder.parameters(), learning_rate)\n",
        "  dec_optimizer = torch.optim.Adam(decoder.parameters(), learning_rate)\n",
        "  loss_fn = nn.CrossEntropyLoss(reduction = 'sum')\n",
        "  encoder.train()\n",
        "  decoder.train()\n",
        "  loss_list = []\n",
        "  for i in range(epochs):\n",
        "    loss, encoder, decoder = train(input_data, loss_fn, enc_optimizer, dec_optimizer, encoder, decoder, num_of_enc_layers, num_of_dec_layers, cell_type)\n",
        "    loss_list.append(loss/51200)\n",
        "    print(loss)\n",
        "\n",
        "  #train_prediction by eval\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ifkAGoezFiwH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Experimental Area**"
      ],
      "metadata": {
        "id": "6h-qHdB__elc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = len(max(df['eng'], key=len))"
      ],
      "metadata": {
        "id": "YnM5xI7J_uUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ic1s-nqVAHFH",
        "outputId": "b109c392-3b67-48e7-e255-0dfd5cadafef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df['eng']:\n",
        "  if(len(i) > 24):\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "f8o8cuGzAInq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = [1,2,3,4]\n",
        "a.append(0*4)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OpbtU_mARaD",
        "outputId": "d4b280f2-19fc-45fa-b058-bc99b4e9d6c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ZO63eD6IhUu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}