{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RutujKhare1/CS6910_Assignment3/blob/main/FDL_A3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nunHnybCpgc",
        "outputId": "be4475a1-4dad-4ae0-abbf-af4c4df95257"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.31)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.21.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0EEiw_RlOn4"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2XJa6S3Yk4B"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import random\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZG0Efld9B2w",
        "outputId": "66a2b409-d1dc-4fdf-fd62-5692a5c66f9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    device_name = torch.device(\"cuda\")\n",
        "else:\n",
        "    device_name = torch.device('cpu')\n",
        "print(\"Using {}.\".format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ov3SdFg2rxew",
        "outputId": "4b99872c-35dd-421b-9ec0-17e3d06ab70a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6srSwMqgClUB"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/FDL_A3/hin/hin_train.csv', names=['eng','hin'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U60nMHdVRGEw"
      },
      "outputs": [],
      "source": [
        "eng_words = df['eng'].copy()\n",
        "eng_maxlen = len(max(df['eng'], key=len))\n",
        "for i in range(len(eng_words)):\n",
        "  l = len(eng_words[i])\n",
        "  eng_words[i] = eng_words[i] + \"*\"*(eng_maxlen - l + 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byPgiy6fVqfq"
      },
      "outputs": [],
      "source": [
        "hin_words = df['hin'].copy()\n",
        "hin_maxlen = len(max(df['hin'], key=len))\n",
        "for i in range(len(hin_words)):\n",
        "  l = len(hin_words[i])\n",
        "  hin_words[i] = \"#\" + hin_words[i] + \"*\"*(hin_maxlen - l + 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QBfA5AnqbdH"
      },
      "outputs": [],
      "source": [
        "unique_eng_letters = set(''.join(eng_words))\n",
        "unique_hin_letters = set(''.join(hin_words))\n",
        "int_to_eng = dict(enumerate(unique_eng_letters))\n",
        "eng_to_int = {char: ind for ind, char in int_to_eng.items()}\n",
        "\n",
        "int_to_hin = dict(enumerate(unique_hin_letters))\n",
        "hin_to_int = {char: ind for ind, char in int_to_hin.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u36AijnMRBbB"
      },
      "outputs": [],
      "source": [
        "index_eng_words = []\n",
        "for eng_word in eng_words:\n",
        "  index_eng_word = [eng_to_int[i] for i in eng_word]\n",
        "  index_eng_words.append(index_eng_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBUP1K6jolVb"
      },
      "outputs": [],
      "source": [
        "index_hin_words = []\n",
        "for hin_word in hin_words:\n",
        "  index_hin_word = [hin_to_int[i] for i in hin_word]\n",
        "  index_hin_words.append(index_hin_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaHcaexBFjO2",
        "outputId": "4aa7f878-11ab-4ce9-dd51-2d2bc4b7ece3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[60, 18, 39,  ..., 23, 23, 23],\n",
              "        [60, 13,  3,  ..., 23, 23, 23],\n",
              "        [60, 56,  3,  ..., 23, 23, 23],\n",
              "        ...,\n",
              "        [60,  1, 39,  ..., 23, 23, 23],\n",
              "        [60, 39, 12,  ..., 23, 23, 23],\n",
              "        [60,  1, 17,  ..., 23, 23, 23]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "tensor_eng = torch.tensor(index_eng_words).to(device_name)\n",
        "tensor_hin = torch.tensor(index_hin_words).to(device_name)\n",
        "tensor_hin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4q529z-FjCx"
      },
      "outputs": [],
      "source": [
        "class GRU_Encoder(nn.Module):\n",
        "  def __init__(self, input_size, hid_size, num_of_enc_layers, emb_size, batch_size, dropout, bi_direct):\n",
        "    super(GRU_Encoder, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hid_size = hid_size\n",
        "    self.num_of_enc_layers = num_of_enc_layers\n",
        "    self.emb_size = emb_size\n",
        "    self.batch_size = batch_size\n",
        "    self.bi_direct = bi_direct\n",
        "    self.dropout = dropout\n",
        "    self.embedding = nn.Embedding(input_size, emb_size)\n",
        "    # print(\"IS:{} ES:{}\".format(input_size, emb_size))\n",
        "    self.gru = nn.GRU(emb_size, hid_size, num_of_enc_layers, bidirectional = bi_direct, dropout = dropout)\n",
        "\n",
        "  def forward(self, input_data, hidden):\n",
        "    input_data = input_data.T\n",
        "    # print(input_data.shape)\n",
        "    embed = self.embedding(input_data).to(device_name)\n",
        "    # print(embed.shape,hidden.shape)\n",
        "    # embed = embed.view(-1, self.batch_size, self.hid_size)\n",
        "    output, hidden = self.gru(embed, hidden)\n",
        "    # if(self.bi_direct):\n",
        "    #   print(\"bir\\n\")\n",
        "    #   hidden = hidden.resize(2, self.num_of_enc_layers, self.batch_size, self.hid_size)\n",
        "    #   print(hidden.shape)\n",
        "    #   hidden = torch.add(hidden[0], hidden[1])/2\n",
        "    #   print(hidden.shape)\n",
        "    return output, hidden\n",
        "  \n",
        "  def initialiseHidden(self):\n",
        "    if(self.bi_direct):\n",
        "      return torch.zeros(2*self.num_of_enc_layers, self.batch_size, self.hid_size, device = device_name)\n",
        "    else:\n",
        "      return torch.zeros(self.num_of_enc_layers, self.batch_size, self.hid_size, device = device_name)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDr5Mjb6Fi0a"
      },
      "outputs": [],
      "source": [
        "class GRU_Decoder(nn.Module):\n",
        "  def __init__(self, op_size, num_of_dec_layers, hid_size, batch_size, emb_size, dropout, bi_direct):\n",
        "    super(GRU_Decoder, self).__init__()\n",
        "    self.op_size = op_size\n",
        "    self.hid_size = hid_size\n",
        "    self.num_of_dec_layers = num_of_dec_layers\n",
        "    self.emb_size = emb_size\n",
        "    self.batch_size = batch_size\n",
        "    self.bi_direct = bi_direct\n",
        "    self.embedding = nn.Embedding(op_size, emb_size)\n",
        "    self.op = nn.Linear(2*hid_size, op_size) if (bi_direct) else nn.Linear(hid_size, op_size)\n",
        "    self.softmax = nn.LogSoftmax(dim = 2)\n",
        "    self.gru = nn.GRU(emb_size, hid_size, num_of_dec_layers, bidirectional = bi_direct, dropout = dropout)\n",
        "\n",
        "  def forward(self, input_data, hidden):\n",
        "    # print(input_data)\n",
        "    embed = self.embedding(input_data)\n",
        "    embed = embed.view(-1, self.batch_size, self.emb_size)\n",
        "    out, hidden = self.gru(embed, hidden)\n",
        "    # print(out.shape)\n",
        "    temp = self.op(out)\n",
        "    out = self.softmax(temp)\n",
        "    return out, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KToV_mC2WtPI"
      },
      "outputs": [],
      "source": [
        "def train(input_data, target_data, loss_fn, enc_optimizer, dec_optimizer, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, cell_type):\n",
        "  teacher_forcing = 0.5\n",
        "  loss = 0\n",
        "  for b in range(0, len(input_data), batch_size):\n",
        "    x, y = input_data[b : b+batch_size], target_data[b : b+batch_size]\n",
        "    temp = 0\n",
        "    enc_optimizer.zero_grad()\n",
        "    dec_optimizer.zero_grad()\n",
        "    # x = x.T\n",
        "    # y = y.T\n",
        "    t_step = len(x)\n",
        "    if(cell_type == 'GRU'):\n",
        "      enc_hidden = encoder.initialiseHidden()\n",
        "      enc_output, enc_hidden = encoder(x, enc_hidden)\n",
        "      \n",
        "      if(num_of_dec_layers > num_of_enc_layers):\n",
        "        num = num_of_dec_layers\n",
        "        dec_hidden = enc_hidden\n",
        "        while(num != num_of_enc_layers):\n",
        "          dec_hidden = torch.cat([dec_hidden, enc_hidden[-1].unsqueeze(0)], dim = 0)\n",
        "          num -= 1\n",
        "      elif(num_of_dec_layers < num_of_enc_layers):\n",
        "        dec_hidden = enc_hidden[-num_of_dec_layers:]\n",
        "      else:\n",
        "        dec_hidden = enc_hidden\n",
        "      y = y.T\n",
        "      dec_input = y[0]\n",
        "      # print(\"Decoder IP : {}\\nDecoder Hidden : {}\".format(dec_input.shape, dec_hidden.shape))\n",
        "      condition = False if random.random() > teacher_forcing else True\n",
        "      if(condition):\n",
        "        for i in range(1,len(y)):\n",
        "          dec_output, dec_hidden = decoder(dec_input, dec_hidden)\n",
        "          temp += loss_fn(torch.squeeze(dec_output), y[i])\n",
        "          dec_input = y[i]\n",
        "      else:\n",
        "        for i in range(1,len(y)):\n",
        "          dec_output, dec_hidden = decoder(dec_input, dec_hidden)\n",
        "          prob, idx = dec_output.topk(1)\n",
        "          temp += loss_fn(torch.squeeze(dec_output), y[i])\n",
        "          dec_input = idx\n",
        "    temp.backward()\n",
        "    enc_optimizer.step()\n",
        "    dec_optimizer.step()\n",
        "    loss += temp\n",
        "  # print(len(target_data))\n",
        "  return loss.item() / len(target_data), encoder, decoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tosOCKpmU49G"
      },
      "outputs": [],
      "source": [
        "def eval(input_data, target_data, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, cell_type):\n",
        "  out = []\n",
        "  for b in range(0, len(input_data), batch_size):\n",
        "    x, y = input_data[b : b+batch_size], target_data[b : b+batch_size]\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    predicted_data = list()\n",
        "    # x = x.T\n",
        "    # y = y.T\n",
        "    t_step = len(x)\n",
        "    if(cell_type == 'GRU'):\n",
        "      enc_hidden = encoder.initialiseHidden()\n",
        "      enc_output, enc_hidden = encoder(x, enc_hidden)\n",
        "      \n",
        "      if(num_of_dec_layers > num_of_enc_layers):\n",
        "        num = num_of_dec_layers\n",
        "        dec_hidden = enc_hidden\n",
        "        while(num != num_of_enc_layers):\n",
        "          dec_hidden = torch.cat([dec_hidden, enc_hidden[-1].unsqueeze(0)], dim = 0)\n",
        "          num -= 1\n",
        "      elif(num_of_dec_layers < num_of_enc_layers):\n",
        "        dec_hidden = enc_hidden[-num_of_dec_layers:]\n",
        "      else:\n",
        "        dec_hidden = enc_hidden\n",
        "      y = y.T\n",
        "      dec_input = y[0]\n",
        "\n",
        "      for i in range(len(y)):\n",
        "        dec_output, dec_hidden = decoder(dec_input, dec_hidden)\n",
        "        prob, idx = dec_output.topk(1)\n",
        "        idx = idx.squeeze()\n",
        "        dec_input = idx\n",
        "        predicted_data.append(idx.tolist())\n",
        "      out.append(predicted_data)\n",
        "  return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifkAGoezFiwH"
      },
      "outputs": [],
      "source": [
        "def training(input_data, input_size, target_data, target_size, max_input_size, epochs, batch_size, emb_size, num_of_enc_layers, num_of_dec_layers, hid_size, cell_type, bi_direct, enc_dropout, dec_dropout, beam_size):\n",
        "  learning_rate = 0.001\n",
        "  if(cell_type == \"GRU\"):\n",
        "    encoder = GRU_Encoder(input_size, hid_size, num_of_enc_layers, emb_size, batch_size, enc_dropout, bi_direct).to(device_name)\n",
        "    decoder = GRU_Decoder(target_size, num_of_dec_layers, hid_size, batch_size, emb_size, dec_dropout, bi_direct).to(device_name)\n",
        "  \n",
        "  enc_optimizer = torch.optim.Adam(encoder.parameters(), learning_rate)\n",
        "  dec_optimizer = torch.optim.Adam(decoder.parameters(), learning_rate)\n",
        "  loss_fn = nn.CrossEntropyLoss(reduction = 'sum')\n",
        "  encoder.train()\n",
        "  decoder.train()\n",
        "  loss_list = []\n",
        "  for i in range(epochs):\n",
        "    loss, encoder, decoder = train(input_data, target_data, loss_fn, enc_optimizer, dec_optimizer, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, cell_type)\n",
        "    loss_list.append(loss/51200)\n",
        "    print(\"Epoch : {} \\tLoss : {}\".format(i, loss))\n",
        "\n",
        "  return encoder, decoder, num_of_enc_layers, num_of_dec_layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Us7xCAvmwhbE",
        "outputId": "2a475a4b-106d-4958-f7bc-f1df04d947cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0 \tLoss : 23.91609375\n",
            "Epoch : 1 \tLoss : 13.3956982421875\n",
            "Epoch : 2 \tLoss : 10.218072509765625\n",
            "Epoch : 3 \tLoss : 8.756336059570312\n",
            "Epoch : 4 \tLoss : 8.148068237304688\n",
            "Epoch : 5 \tLoss : 7.40233154296875\n",
            "Epoch : 6 \tLoss : 6.897642822265625\n",
            "Epoch : 7 \tLoss : 6.452665405273438\n",
            "Epoch : 8 \tLoss : 6.10385986328125\n",
            "Epoch : 9 \tLoss : 5.775126953125\n",
            "Epoch : 10 \tLoss : 5.308073120117188\n",
            "Epoch : 11 \tLoss : 5.147074584960937\n",
            "Epoch : 12 \tLoss : 5.013026733398437\n",
            "Epoch : 13 \tLoss : 4.829122924804688\n",
            "Epoch : 14 \tLoss : 4.34072021484375\n",
            "Epoch : 15 \tLoss : 4.1283642578125\n",
            "Epoch : 16 \tLoss : 4.005160217285156\n",
            "Epoch : 17 \tLoss : 3.856758728027344\n",
            "Epoch : 18 \tLoss : 3.7322817993164064\n",
            "Epoch : 19 \tLoss : 3.425146484375\n"
          ]
        }
      ],
      "source": [
        "encoder, decoder, num_of_enc_layers, num_of_dec_layers = training(input_data = tensor_eng, \n",
        "                                                                  input_size = len(unique_eng_letters), \n",
        "                                                                  target_data = tensor_hin, \n",
        "                                                                  target_size = len(unique_hin_letters), \n",
        "                                                                  max_input_size = 24, \n",
        "                                                                  epochs = 20, \n",
        "                                                                  batch_size = 64, \n",
        "                                                                  emb_size = 32, \n",
        "                                                                  num_of_enc_layers = 3, \n",
        "                                                                  num_of_dec_layers = 1, \n",
        "                                                                  hid_size = 256, \n",
        "                                                                  cell_type = \"GRU\", \n",
        "                                                                  bi_direct = False, \n",
        "                                                                  enc_dropout = 0,\n",
        "                                                                  dec_dropout = 0.2, \n",
        "                                                                  beam_size = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kejVozOEl_o"
      },
      "outputs": [],
      "source": [
        "trained_pred = eval(tensor_eng, tensor_hin, encoder, decoder, num_of_enc_layers, num_of_dec_layers, 64, \"GRU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyHQWp5kI_gP"
      },
      "outputs": [],
      "source": [
        "temp = torch.tensor(trained_pred)\n",
        "# temp.shape\n",
        "temp = temp.view(23, 51200)\n",
        "pred_up = temp.T.to(device_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hin_maxlen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7cyXu1OcIx0",
        "outputId": "b6f77907-8198-49c8-eb4e-3946b47637d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kA10NUNLaqR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21a4d7a9-0f74-4650-c6d5-8877cdef6f92"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[18, 23, 23,  ..., 23, 23, 65],\n",
              "        [13, 23, 23,  ..., 23, 23, 38],\n",
              "        [56, 23, 23,  ..., 23, 23, 57],\n",
              "        ...,\n",
              "        [23, 23, 58,  ..., 23, 42, 23],\n",
              "        [23, 23, 17,  ..., 62, 22, 23],\n",
              "        [23,  3, 57,  ..., 65, 58, 23]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "pred_up"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculateAccuracyUPD(y_pred, y_true):\n",
        "  cnt = 0\n",
        "  for i,j in zip(y_pred, y_true):\n",
        "    cor = torch.eq(i, j)\n",
        "    if(torch.mean(cor.float()).item() == 1.0):\n",
        "      cnt += 1\n",
        "  return cnt / len(y_pred)"
      ],
      "metadata": {
        "id": "WuZjjLpFfAzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculateAccuracyUPD(pred_up, tensor_hin)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0DfQkwvfmDD",
        "outputId": "0218591e-ab57-4db0-ee90-2555c7c6581a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_up[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlBRhNblgELx",
        "outputId": "a7808a7c-901d-462b-acd6-88121458d5a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([23])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF2NuCcNDKZc",
        "outputId": "fc9cd517-89f0-4777-c13f-55e85f5488f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4N6_to6CIrp"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv('/content/drive/MyDrive/FDL_A3/hin/hin_test.csv', names=['eng','hin'])\n",
        "eng_words = df_test['eng']\n",
        "maxlen = len(max(df['eng'], key=len))\n",
        "index_eng_words = []\n",
        "for eng_word in eng_words:\n",
        "  index_eng_word = [eng_to_int[i] for i in eng_word]\n",
        "  l = len(index_eng_word)\n",
        "  index_eng_word.extend([0]*(maxlen-l+2))\n",
        "  index_eng_words.append(index_eng_word)\n",
        "\n",
        "tensor_eng_test = torch.tensor(index_eng_words).to(device_name)\n",
        "tensor_hin_test = torch.tensor(index_hin_words).to(device_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "rBsT_3WzDSg-",
        "outputId": "77a1f39a-8e11-4577-9464-6f94980454cd"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-b82a46ffc905>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_eng_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_hin_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: eval() missing 5 required positional arguments: 'encoder', 'decoder', 'num_of_enc_layers', 'num_of_dec_layers', and 'cell_type'"
          ]
        }
      ],
      "source": [
        "pred_value = eval(tensor_eng_test, tensor_hin_test, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h-qHdB__elc"
      },
      "source": [
        "# **Experimental Area**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnM5xI7J_uUt"
      },
      "outputs": [],
      "source": [
        "maxlen = len(max(df['eng'], key=len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ic1s-nqVAHFH"
      },
      "outputs": [],
      "source": [
        "len(tensor_eng)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8o8cuGzAInq"
      },
      "outputs": [],
      "source": [
        "for i in df['eng']:\n",
        "  if(len(i) > 24):\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OpbtU_mARaD"
      },
      "outputs": [],
      "source": [
        "a = [1,2,3,4]\n",
        "a.append(0*4)\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_words"
      ],
      "metadata": {
        "id": "pcFlF96qnMGL",
        "outputId": "e3b2efca-866d-4306-daa5-d1d08c047626",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        shastragaar***************\n",
              "1        bindhya*******************\n",
              "2        kirankant*****************\n",
              "3        yagyopaveet***************\n",
              "4        ratania*******************\n",
              "                    ...            \n",
              "51195    toned*********************\n",
              "51196    mutanaazaa****************\n",
              "51197    asahmaton*****************\n",
              "51198    sulgaayin*****************\n",
              "51199    anchuthengu***************\n",
              "Name: eng, Length: 51200, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZO63eD6IhUu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dccff7a9-a466-4bd1-eb75-b8d6283154a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "श***र***ान***श***य्***द\n",
            "ब***ल***बी***ा***ंल***ई\n",
            "क**ी्***ज्**या****न***्\n",
            "य***ी***ि्***त***िँ***्\n",
            "र***ं****ो***ी****र***ी\n",
            "व***स**ल*र**ली****म***ी\n",
            "द**ाे****श***स***््***ा\n",
            "स***क***ैा**लट***ीं**व्\n",
            "म**ाप***लु*******िं****\n",
            "स**गा***ों**ंव***डल***न\n",
            "ब***ग****क**मन***रथ***्\n",
            "त***े***न्**ुश****ब***ि\n",
            "क**घ्****ज**ीु***िर***ड\n",
            "इ***ौ***गा***क****इ***स\n",
            "म**ेट***एब**चट***ीस***ग\n",
            "अ**रम***़ु***ी***यन**व*\n",
            "अ**कड***ा़***ग****ज***ा\n",
            "ध**गद****्***न****र**ो*\n",
            "अ***े***रै***य****र***ं\n",
            "ए***े***ाे*******ाल***ढ\n",
            "ब***म****ं*******दम***र\n",
            "ब***ी***ेो***ह****प***ू\n",
            "प***स***ील**रो***िस***ि\n",
            "ह***े****श***य***यल***ई\n",
            "ज***त****स***ी****ट***ं\n",
            "प**कभ***टी***र***शक***ा\n",
            "ज***ी***ोू**रय***यर***क\n",
            "प***म***का***ट****र***न\n",
            "आ*******वि***ा***ुक**़क\n",
            "व**गन***तस***ल***लज***ा\n",
            "च***्****े***ु***डश***र\n",
            "म***र****े***व****्**्स\n",
            "अ***ट****ं***क****्***ा\n",
            "इ***ा***कव**यो****त***स\n",
            "द**दू***टु***प***मि***ह\n",
            "स***ै****स********ष***र\n",
            "श**ठा**ीाक********स****\n",
            "उ***त***वि********ि***े\n",
            "अ***्****न***ा****व***े\n",
            "अ***ग***ाॉ***ं****च***प\n",
            "ख**वड***्ौ***व***ी़***ो\n",
            "थ**प्***मा***म***ंड*न*ा\n",
            "स***ी***कह***ा***तए***ज\n",
            "ई*******लि**ं***र*ा***र\n",
            "भ**हो***सि***ड***ीस***ी\n",
            "इ***ा***ेथ***ं****म***म\n",
            "छ***भ**मणा***ो****श**ता\n",
            "त***ल***मा***ि****्***र\n",
            "ग**लत****्***े***यस**रि\n",
            "व***्***दो***र***हु**य*\n",
            "थ***्**ंकप***ब***ीद***ए\n",
            "स***य****ॉ********क*श**\n",
            "प***क***न्***ओ****्***ल\n",
            "क***ो***रर***ि***रर***स\n",
            "अ***ा****ह***ा***यग***य\n",
            "व***ट****ि***य****ह***़\n",
            "आ***ि***यप*******रक***स\n",
            "प***े****ि**बट****न***प\n",
            "प**्ा**ाेम***ट****स***ष\n",
            "म***न****ा***श***दा***श\n",
            "व**टा***गी***य****र**ो्\n",
            "र***प***ंआ***व***्स****\n",
            "न***ा***न्***ॉ****ब**ाु\n",
            "क**सा***रय**तप****ु**्*\n",
            "स***ि***ले****म***र***्\n",
            "ि***्***ीए***नन**चा****\n",
            "ि***ध***नर**ेकक***े***क\n",
            "ज***प***यल***ाद**दग***र\n",
            "त***ड****ल***गफ***ी****\n",
            "ग***ट**ल*ह**ि*द***त****\n",
            "े**इं****ि***ील**्र***ल\n",
            "ु***े***पश***ीघ**ठग**ाय\n",
            "ो**रक***ूं****ख**कग****\n",
            "र*******ंग***रझ***ि***ी\n",
            "स***े****ड**ि*ब**ेै***य\n",
            "ु***ख***ार**स्स***ि***त\n",
            "ा***र****ा***खम**सब***्\n",
            "न***र****ँ***ात***स****\n",
            "ै**ं****ना**ााक**वे****\n",
            "ग**हो****आ****ब**मि**्*\n",
            "न**ेि***इो***ेह***ौ***र\n",
            "ू**ेु****र***्फ***्**ं*\n",
            "व***झ***तन***ास***ि****\n",
            "ब***म****ज****ह***ि***़\n",
            "ै***ज****ढ****प**ुा****\n",
            "ा***फ***मह***ोय***ड***र\n",
            "ं***ए****प**णमश**वे***य\n",
            "ै***श****ु***ाए**ाी****\n",
            "व***ि****्***लव***प***ग\n",
            "ै**ाल***ोश***नश**नफ****\n",
            "ग***व***ंम***ाव**ोा***ि\n",
            "ो***ा****र***्ड***ै****\n",
            "ई********ट***दन**का**ाल\n",
            "े**रा***ेा****अ**ोी****\n",
            "ि***य****श***कज***च****\n",
            "ा***ण****प***ूग***न**त*\n",
            "न***ब****ग***्इ***ड****\n",
            "स***इ***्र**े*इ***ा***ए\n",
            "ु***न***ीर***नस**ान***स\n",
            "ि***न****न****व***्***ा\n",
            "ि**ाप***र्****फ***े****\n",
            "ज***ा***रन****झ***य***व\n",
            "प***ल****्***लख***ि***ल\n",
            "म***ा****र***चस***क***ु\n",
            "ग**ि़***रर***हस***ब***र\n",
            "ि**ूस***ील****प***े****\n",
            "ो***आ***ास****न***फ***ह\n",
            "प*******ेश**च*व***म****\n",
            "ा***ग***्ल***़न***्****\n",
            "स***न***टप***तन***ग****\n",
            "ा***ू***जथ***जर***्**ील\n",
            "ो***न***ार***नर***म***ि\n",
            "ु**ी्****र***लज**ार**ीन\n",
            "ि***ष***िड****उ**ाद****\n",
            "र***न****्***लक***्***ं\n",
            "ा***ा****न****म***ू****\n",
            "ि***ो***ार***ंक***म***प\n",
            "ै***न****ै***तत**मै***न\n",
            "र***म****र***लन**कि***ा\n",
            "ा***ट****व***ाआ***न***े\n",
            "ई***न***ोी****ब***र***ट\n",
            "ि********ं**रेर***ा***ी\n",
            "ह**तन**बला***ोज***र***ि\n",
            "ै***ा****र***ेश**ीच***ी\n",
            "ि**्व***ीआ***ाइ***्**ोव\n",
            "ु***ि***कर***ंअ**शर****\n",
            "ज***र***िय***ंश***ै**वर\n",
            "्**िर***्क**ुरभ***क**ध*\n",
            "्***तज**ोे****ो***ि***र\n",
            "ं***मस***म****ि**ीह****\n",
            "र***िअ**ने***्ै***ं***ी\n",
            "्***तअ**ास***नम**ाी***ो\n",
            "ा***वव***ा***े्***ग****\n",
            "न***्व*े*ह**श*े***ि****\n",
            "श**डफव***म***ला**रप****\n",
            "घ***टज***्***सल***र***ा\n",
            "ह**ी्ए***ज****र**ाब****\n",
            "्****क***ल***्ि***य****\n",
            "े***रअ***म**ं*ी**ीि***ा\n",
            "म***रक**पी**ावि***त****\n",
            "न***पज***म***रा**्स***य\n",
            "ट***ाब***ध***ली***ि****\n",
            "च****ब***ऊ**र*ा***म****\n",
            "र**यनग***न****च***च**क*\n",
            "ु**शनह**जक****ा***र***ा\n",
            "ल***लव***म***तॉ***ट****\n",
            "ल***रइ**्न***नै***स****\n",
            "र***नच***र****ै***ह****\n",
            "ल***ील***क****ट**लम****\n",
            "व***ास***ा****ू***़***प\n",
            "क***मह***्***िा**ि****न\n",
            "ं***्घ***भ****ह**ाप****\n",
            "ा***कख***ट****ि***े***े\n",
            "ं**ठेआ**ले****ब***्****\n",
            "न***िन***ि***ंह**ंस***श\n",
            "न***हस***्***रे***श****\n",
            "ए****स***ा***ुद**्य***ा\n",
            "द**्मम**चथ****प***ड****\n",
            "न***ास***ा***्ो***ं****\n",
            "र****अ*******डद***ा****\n",
            "स***ाए***ल***सम***ल****\n",
            "्***िद**टा****ं***ध***म\n",
            "ः****व**शे***क्**ीा***्\n",
            "घ***कग***ा****ि***य****\n",
            "क**ठ्ल***ष****ै***च****\n",
            "व***नब**ीी****क***ा***ा\n",
            "ट***ाप***ह***ुा***ल***म\n",
            "्***ंल***्***ाम***ि***र\n",
            "न**कीस**िम****्***ा***्\n",
            "य**ट*ब***ा****ढ***स****\n",
            "श***ईज***्****ै***ड***े\n",
            "*****क**श्**्*े***स****\n",
            "ज****द**टच***ाि***क****\n",
            "्***ान**िु***ीा***ो****\n",
            "य****न**ंे****ि***च**चप\n",
            "प****भ**र्***ीि***ि***य\n",
            "ड***रक***े***श़**दा***ो\n",
            "स***कप***ि****च**ँ्****\n",
            "्***्ज***र***ीु***ध****\n",
            "ं***तर***ल****े***ट****\n",
            "ं***सक**इस***सै***म***ु\n",
            "ं***ीम***ल****े**ां****\n",
            "े***ाइ***म***सि***न****\n",
            "ह***मच***ा****ज***त****\n",
            "ट****आ***ब****ि*******ा\n",
            "क****त***द***लण***ह***र\n",
            "न**ााल*लाज***ला***ज***त\n",
            "ड***तक***्***ंा***ा****\n",
            "द**सतप***ई***ंव***क****\n",
            "प***नप**लव***तभ**ील****\n",
            "र***नश**कू***चे******िद\n",
            "र**न्ल**वल**मचट***ा****\n",
            "त****ो**सव****न***सअ***\n",
            "ध***्ु***आ****य***ीब***\n",
            "ण***कफ***ड***रथ****प***\n",
            "ञ***्त***्***ेत**रंह**म\n",
            "न***रि***ट***टर***ोम***\n",
            "्***रि***ी**ि*व****क***\n",
            "भ***्े***ी***िल**मकत***\n",
            "ड****ी***य***ीौ***ाह***\n",
            "ी***वक***त****ी***ित***\n",
            "व****य***ि***डह***रन***\n",
            "क***ेल***ा**ट*क***िब**च\n",
            "च***ाा**ीघ**ररड****स***\n",
            "्***िर***य****य**त्भ**ा\n",
            "ा****ु***्***शर***ंस***\n",
            "्****्***ं**्*ल****प***\n",
            "ी**्ेर***्****ा***्च***\n",
            "क**न*े**्*****इ***ाओ**न\n",
            "च****ि***त***िर***ोय***\n",
            "े***चग***ै***ीन****म***\n",
            "ा***िि***्****ं****क***\n",
            "र****ा***े****्***ेन***\n",
            "ल***यै***ल****न****च**त\n",
            "्***ईा***स***कख**य*स***\n",
            "क***वट***ो****ु**ा्क***\n",
            "न***र्***्****ं***टक***\n",
            "क**ा*ठ**ा*****र***रर***\n",
            "ध***जल***य***तर***ाख**्\n",
            "्***ाु***क***ोब***कद***\n",
            "न****ु***न***ली**ताल**ओ\n",
            "ल**्ैग**ेो****न***ॉव***\n",
            "व***कक***त***तह***दम***\n",
            "व****प*******ची***सअ***\n",
            "म****स***ि****प***ेब***\n",
            "ट***यक********ड***िद**ए\n",
            "ख****र**नश****ट**ीयघ**ट\n",
            "ा***ोज***थ****फ***कड***\n",
            "्**ीरख***म****स***ीअ***\n",
            "ा****द**लश****झ****द**ल\n",
            "्***रि***ा****ं****क***\n",
            "ब***वो***ट****ज***यइ**ा\n",
            "ी****ी**या****प***तक**स\n",
            "ा**ी*न***थ****़****फ***\n",
            "ि***आु***त****न***ीह**द\n",
            "*****ा***व**य*द***िब***\n",
            "प****ु***ि****ल***रव***\n",
            "त****ा**ंत****म***डप***\n",
            "ा****ृ**कर****प***यझ*्ु\n",
            "व****ा***ड****ट***यआ**ा\n",
            "़***ाड***श***्ा**्वक**स\n",
            "ल***ार***फ****ा***दब***\n",
            "म***ये***क****म***वम***\n",
            "च***ंि***े****ग***रक***\n",
            "ट***िा**ज्***ेक***ूर**र\n",
            "ट****ि***ी****ं**णकक***\n",
            "ं***रं***े***सप***ोइ***\n",
            "ि***्प***स****ी***्ए***\n",
            "ी****इ***ी****ज****इ**ल\n",
            "्****ा***ु***ंु***ेइ**ि\n",
            "ा****ै*द*ा***ाय***ाच***\n",
            "स***ेम***ट****ब***रव***\n",
            "य***ी्***ए***वि***ीर***\n",
            "य****र***ी***ीी**ंीअ***\n",
            "ब****म***र****र****स*सा\n",
            "ी***टा**िश**ाा्***बक***\n",
            "्****ब***रम***ो***ीप***\n",
            "्***मर***ईन***म****े***\n",
            "क***ीर***िन**ाव****द***\n",
            "ो***र्***टअ***ु**ी*ि**ा\n",
            "ि***्ख***िव***े***डज***\n",
            "य***ैर***ंअ*ट*व****ो***\n",
            "र***टस****ब**कर***्र***\n",
            "़****व***ार***ट***जर***\n",
            "व***नी***ाव***द***लि***\n",
            "स****्***शस***ा****ि***\n",
            "ो****फ****भ*ल*े***कल**े\n",
            "य****र***ाच**ने****े***\n",
            "य***कह***ेल***ा**ाध्***\n",
            "क****ल***यच**ो्***गे***\n",
            "य****र***गर*य*म****्***\n",
            "***मलब***हब***व***के***\n",
            "्****ज**ड*च***प****क**ी\n",
            "ै****च***िआ**क्***ला***\n",
            "ह***ी्***नर***्****ू***\n",
            "र***टत***टम***ड****्***\n",
            "्****इ****ल***ट***लै***\n",
            "ी***रग***पब***ि****ि**ा\n",
            "च***सउ***पप**्े**न*ा***\n",
            "र***रक***ंन***ग**ारं***\n",
            "ो***णय***रम***ज***्ह***\n",
            "ी****ब**उ*उ*******ेफ***\n",
            "ा***न्***ाज**रर***ठो**व\n",
            "न****न***ाच**प्***ोा***\n",
            "ब****प***ीस**ाब***पि**ं\n",
            "म**त*र***पऊ***ा***नि***\n",
            "ा***ाल***हस***न****ै***\n",
            "ा****्****फ**ीर***ान***\n",
            "झ****ड***िस***र***कच***\n",
            "ू***ा्****न***ि***कि**ल\n",
            "ी****्**्पह***ड***ाड***\n",
            "ं***वे***पय***फ***ोै***\n",
            "ष***का***णच***्****र***\n",
            "ल****ा***ाध***ो****ु***\n",
            "ट***ाल***वव***ड****न***\n",
            "ि****ट***िज***व***ेद***\n",
            "*****र****ब***ल***ीौ**ड\n",
            "म****न***िव***व****ट***\n",
            "य***ईत***्द***ि****क***\n",
            "*****ई***ाग*ा*ा***यि***\n",
            "क****ल***सब***े***्ु***\n",
            "ि****र***ेग***स***ां***\n",
            "ट****त****ड***ो***वा*यर\n",
            "ा****प***डम***्***ाव***\n",
            "ा***भव***डप**वर**देा***\n",
            "े***ंर***ाव***न***ीन***\n",
            "ा***्र***ाअ***श***ेा***\n",
            "े***ंं***नप***ा***वू***\n",
            "ि***यर**्तह***त***ला***\n",
            "ा****त****य***द**ेोै***\n",
            "ग***्ट***ंप**ा्***वन***\n",
            "न***टल***ाब***व***यस***\n",
            "प****ज***एच***ा****म**ा\n",
            "स****य***सग**टब***डं**ड\n",
            "ऊ****ग****प***द***नि***\n",
            "े****ब***शफ***फ***्ा***\n",
            "ा****य***सच**वं****स***\n",
            "े****ि****ग***त****न***\n",
            "ं****र***ोन***ा****ु*ोह\n",
            "च****व**त*ठ**लट***ल्***\n",
            "रम***ि****ि***क****ू***\n",
            "यन**ाु***डच***ि****न***\n",
            "ाअ***ी***मि**ंा****ं***\n",
            "मह***य****र***र**स*च**इ\n",
            "यथ**म्***नृ***े***््***\n",
            "ाथ**कस****प*ि*र****ट***\n",
            "ाब**्न****ि**्ि***षं***\n",
            "पछ***न***ते***ी***ीि***\n",
            "ाज***क****ि***ा****ल***\n",
            "ंस***य***चे***ओ****र***\n",
            "*म***ज****ा***ए****्**ह\n",
            "ाब***्***मे**नक****व***\n",
            "ुब**ात***ंा***व**नार***\n",
            "्ल***्***ों**धथ***ात***\n",
            "ूव***ै***ीी***न****र***\n",
            "*ख*ीाा***ेस***क***िय***\n",
            "रप***ी****ा***र****व***\n",
            "ंक***ा****ई**ाम***ोद***\n",
            "ात***न****ि***य****ल***\n",
            "्च**ीा***शृ***ब****र***\n",
            "सत***र****ा***ि****प***\n",
            "*श***ि***ुह***स****ल***\n",
            "रइ***ं***्र**सं****इ***\n",
            "्ड***ा****े******ासड***\n",
            "ंन***ा***ोो***़***चह***\n",
            "*प***ा**स*प*******मा***\n",
            "मल***स****ा**णू***ीद**र\n",
            "िर***ा***ंन**ेल***रख***\n",
            "ीव***्****म**ंा***्म***\n",
            "*त***ु***्र***क****द***\n",
            "सल**रे***ीक***ि****र***\n",
            "डक***र****ॉ*******ठु***\n",
            "ाज***ी***श्***े***रा***\n",
            "डक***ष****ा***य***ाक***\n",
            "तन***च**सुर***ि***ँी***\n",
            "चइ**िट***ुू***ी****ड***\n",
            "ेव**ाा***वे***ब****म***\n",
            "तस***न****्***र****व***\n",
            "नल***्***ीा***ू****ो***\n",
            "कफ***म***नो***ा****्***\n",
            "*श***ं****ो***ल****ड***\n",
            "िश***ो***यो***ा****्***\n",
            "ोह**ए्***रि***ह****ू***\n",
            "*क***ज***कड***च***ास***\n",
            "ोक***्***्ि***श***रआ***\n",
            "थप***ल***ल्***प****ड***\n",
            "ाब***त****ि***र***िल*ाम\n",
            "लब***द***सा***र***ंे***\n",
            "तम**ाा****र**रो**ल*क***\n",
            "रद**चा***इा***ा***नौ***\n",
            "लथ**या****ल***े***शइ***\n",
            "*क**त्****थ***ं***ाप***\n",
            "यश**ाम**डार***ी***कग***\n",
            "बर***्****ु***ु***नर***\n",
            "िक**यक***कू***प****क***\n",
            "्त**सा***ठु***ा****ए***\n",
            "ीख***ा***सै***ल****्**ं\n",
            "लब***ि****ु***ा***़फ**ि\n",
            "ंर***ा****र*******बप***\n",
            "नश***ख***ॉि***ु***्र***\n",
            "पर***ा***आि**ीग****्***\n",
            "चत***न****्********ा***\n",
            "दश***ा***लि***थ****ख*ं*\n",
            "नस***न**्*ा**की***ेर***\n",
            "ाज***न****ल***ोर***ज***\n",
            "*व***च***ीा***तद***ा***\n",
            "ंश***ड***ित**तडम***ज***\n",
            "वे***च****्***्ब*ो*क**न\n",
            "ाो***य****त***डप**यझ***\n",
            "चा**्स****र***मप***ा***\n",
            "मा**रन****ज**संत**ोग***\n",
            "नक***ा***्ं****ल***भ***\n",
            "ला***र****द***रअ***प***\n",
            "गत***ु***ाम****क***्***\n",
            "*ु***र****ग***सम***ल**ी\n",
            "पह***य****म**न्इ***न***\n",
            "ु्**ं*****ब***ाद**वक***\n",
            "से***ल****द***ार***म***\n",
            "रै***ट****ड***ुप***त***\n",
            "*च*न*प****ौ***ाम**नर***\n",
            "मा***ट****र***फश***ि***\n",
            "डु***र****स**मेस***ल***\n",
            "*ो***ो****ं***ान***ध***\n",
            "डा***म***नद***िग***ै***\n",
            "*ि***ो****ह***टर***ट***\n",
            "*ो***प***रत***ेक***्***\n",
            "्स***ड***रत***टप***स***\n",
            "सो***ं****त****क**ेम***\n",
            "कर***त***लड***रम**नु***\n",
            "*ु***र**र*प****द*******\n",
            "*प***ा****फ****क***र***\n",
            "यव***य***डप**नपप**्ल***\n",
            "एि***त****्**गबर**र्***\n",
            "*ी***र***स्***ाइ***्***\n",
            "*ो***श****ो***सच***ि***\n",
            "़ा***ा****ल****क**ीद***\n",
            "*श***ए***्व***ेक***इ***\n",
            "ेम***त****र***ाम**रा***\n",
            "*ि***ु***री***ंग***च***\n",
            "ील**चि***रन***लस***ल***\n",
            "चन**शव***ेत***ेअ***स***\n",
            "म्***ा****य***तग***ि***\n",
            "*ो***ट****ल****द***ह***\n",
            "ा्***ल***ाम****न***र***\n",
            "*ि***त****ह***यव***़***\n",
            "ना***ड***नल***ईथ***ट***\n",
            "लव**एत****व***ाप***ब***\n",
            "*्***़***ाव***रक***्***\n",
            "*ा***ल***पक***सब**ूल***\n",
            "िह***ी****र***्अ***ो***\n",
            "ंक***्****ज***्ख**नर***\n",
            "*ो***्****र***िए***श***\n",
            "ेज**वह****्****स***ा***\n",
            "ीू**्ज***ंल****र***ं***\n",
            "ाे********्***तग**्क***\n",
            "*र**्द****ा***तन**लो***\n",
            "ाा***े***चल****ध***ा***\n",
            "िा***र****ग****स**ीी***\n",
            "ं्**ा****रर***कइ***ॉ***\n",
            "या***न****द***सल***स***\n",
            "एर***इ***ईन***ेत***त**ट\n",
            "मौ***ब****र***ईर**ार**स\n",
            "गे***र****्****प**ाक***\n",
            "*्***्***रल***दर**रद***\n",
            "ीो***ल***ईम***सर***प***\n",
            "ीर***ा****र****प***ल***\n",
            "द्***र***ेट***ाअ***त*ो*\n",
            "*क********क***मक***ो***\n",
            "गह***िर***ि***डा***्***\n",
            "*ा***िव***ऊ***्ा***न***\n",
            "त्***िज**ल्**िार***ा***\n",
            "ीत***ाउ***ह****ह*र*ि***\n",
            "*प***ाआ***त***िि**ाा***\n",
            "ेउ**टिब***ा***नर***ग***\n",
            "धं**िाव***ु***डक**ंो***\n",
            "*ा***नव**यब****म***ा***\n",
            "*इ***नअ***्***ोप***त***\n",
            "््***मख***र****ा***ग***\n",
            "*स***ीक***क****ह***ू***\n",
            "ैि***ाब***्**्टश***्***\n",
            "बर**च*म***े***तर**िु***\n",
            "िय***ुड***्***दं***म***\n",
            "िध***डग***ी***स्***ि***\n",
            "*च***ेर***त***रह***ी***\n",
            "ाप***शत***्***़ि***ल***\n",
            "*व***णन***ी**ीट्***े***\n",
            "*ल***रक***ग***सव***ा***\n",
            "*ल***्ट***ा***लम***ब***\n",
            "*ज***गक***ो***्ख***ा***\n",
            "*न***ोब***्***टै***क***\n",
            "डे****प**वक***रर***ं***\n",
            "*क***कए***्****ा**ंन***\n",
            "ेद***िस**ॉु****े**्ल***\n",
            "*र****क***्****व*******\n",
            "*ल****म***र****ा***ी***\n",
            "मि***ंब**ेि**िरू**से***\n",
            "*र***ीस***ह**लतघ**वब***\n",
            "*व****घ**िध***मल***य***\n",
            "*क***पअ***र***बा***ज***\n",
            "ार***कम***्****ृ**हे***\n",
            "*श***सअ**नर***ट्***य***\n",
            "ंल***ेम***ा***टो**िय***\n",
            "*र***लन**ीब***गढ***ी***\n",
            "*्***यच**ाि***्ं***ॉ***\n",
            "ेल**्तक**रन***ंब***र***\n",
            "*क***मत***ा***ी्***ध***\n",
            "*ह****ब***ु****्*******\n",
            "वर***ोब**इो****र***ि***\n",
            "*व****म***न***ये***ि***\n",
            "*न***कश***्****ा*******\n",
            "जी**एिक***्***ल्*******\n",
            "*व***ाश**पा***ण्***ट***\n",
            "*र***ेक**ीस***ि्***ा***\n",
            "*न***कस***ा***तव***र***\n",
            "ड्***वप***े***डा**या***\n",
            "*इ***वज***्***टल***ज***\n",
            "*ु***टब***स****्***व***\n",
            "*ध**यजद**गर****ि***द***\n",
            "वस****भ***प****्**व्***\n",
            "*व**राव***न***ीव**ेष***\n",
            "*प***सश**ाा****म***ं***\n",
            "लध***घप***ा****ि***स***\n",
            "गर****द**ी्***्ल***ड***\n",
            "ाम***ाह***्***नं***ए***\n",
            "*ा***कक***े***ंो***ै**ि\n",
            "ेज***ाउ***ो****े***्**े\n",
            "ाल***डस***य****ा**बो***\n",
            "*र***तम**््***ीे***त***\n",
            "ठम****क**पट****"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-d9560efaab59>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_up\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_to_hin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;31m# mp.Pool cannot be trusted to flush promptly (or ever),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# wake event thread (message content is ignored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    616\u001b[0m                 )\n\u001b[1;32m    617\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m     def send_multipart(\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for i in pred_up:\n",
        "  for j in i:\n",
        "    print(int_to_hin[j.item()], end=\"\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUdsTZzTs3P3"
      },
      "outputs": [],
      "source": [
        "for x, y in zip(tensor_eng, tensor_hin):\n",
        "  print(x,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4wZuIxvvGHe",
        "outputId": "bdd4e79e-f8a2-48fd-a672-62f1efda3ae7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "len(unique_hin_letters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SK14JJd60Ofz"
      },
      "outputs": [],
      "source": [
        "a = [1,2],[3,4]\n",
        "b = [1,1],[3,4]\n",
        "ta = torch.tensor(a)\n",
        "tb = torch.tensor(b)\n",
        "calculateAccuracyUPD(a,b)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}