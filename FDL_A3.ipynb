{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rutujkhare/fdl-a3-1?scriptVersionId=130561112\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as img\nimport numpy as np\nimport pandas as pd\nimport torch\nimport random\nimport wandb\nimport torch.nn as nn\nimport csv","metadata":{"id":"U2XJa6S3Yk4B","execution":{"iopub.status.busy":"2023-05-22T13:13:10.967884Z","iopub.execute_input":"2023-05-22T13:13:10.968256Z","iopub.status.idle":"2023-05-22T13:13:15.364321Z","shell.execute_reply.started":"2023-05-22T13:13:10.968223Z","shell.execute_reply":"2023-05-22T13:13:15.363384Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device_name = torch.device(\"cuda\")\nelse:\n    device_name = torch.device('cpu')\nprint(\"Using {}.\".format(device_name))","metadata":{"id":"bZG0Efld9B2w","outputId":"66a2b409-d1dc-4fdf-fd62-5692a5c66f9b","execution":{"iopub.status.busy":"2023-05-22T13:13:20.746889Z","iopub.execute_input":"2023-05-22T13:13:20.747258Z","iopub.status.idle":"2023-05-22T13:13:20.816509Z","shell.execute_reply.started":"2023-05-22T13:13:20.747224Z","shell.execute_reply":"2023-05-22T13:13:20.814947Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Using cuda.\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocessingData(df, max_len, eng_to_int, hin_to_int):\n    eng_words = df['eng'].copy()\n    for i in range(len(eng_words)):\n        l = len(eng_words[i])\n        eng_words[i] = eng_words[i] + \"*\"*(max_len - l + 3)\n    hin_words = df['hin'].copy()\n    for i in range(len(hin_words)):\n        l = len(hin_words[i])\n        hin_words[i] = \"#\" + hin_words[i] + \"*\"*(max_len - l + 2)\n\n    index_eng_words = []\n    for eng_word in eng_words:\n        index_eng_word = [eng_to_int[i] for i in eng_word]\n        index_eng_words.append(index_eng_word)\n    index_hin_words = []\n    for hin_word in hin_words:\n        index_hin_word = [hin_to_int[i] if i in hin_to_int else hin_to_int['_'] for i in hin_word]\n        index_hin_words.append(index_hin_word)\n    tensor_eng = torch.tensor(index_eng_words).to(device_name)\n    tensor_hin = torch.tensor(index_hin_words).to(device_name)\n    return tensor_eng, tensor_hin\n","metadata":{"execution":{"iopub.status.busy":"2023-05-22T13:13:21.762883Z","iopub.execute_input":"2023-05-22T13:13:21.763465Z","iopub.status.idle":"2023-05-22T13:13:21.775513Z","shell.execute_reply.started":"2023-05-22T13:13:21.763431Z","shell.execute_reply":"2023-05-22T13:13:21.774521Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def calculateAccuracy(trained_pred, y_true):\n    out = []\n    ten_pred = torch.tensor(trained_pred)\n    for i in range(len(trained_pred)):\n        temp = ten_pred[i].T\n        out.extend(temp)\n    y_pred = torch.stack(out).to(device_name)\n    cnt = 0\n    for i,j in zip(y_pred, y_true):\n        cor = torch.eq(i, j)\n        if(torch.mean(cor.float()).item() == 1.0):\n            cnt += 1\n    return cnt / len(y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T13:13:22.234692Z","iopub.execute_input":"2023-05-22T13:13:22.235496Z","iopub.status.idle":"2023-05-22T13:13:22.243299Z","shell.execute_reply.started":"2023-05-22T13:13:22.235449Z","shell.execute_reply":"2023-05-22T13:13:22.242174Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class GRU_Encoder(nn.Module):\n    def __init__(self, input_size, hid_size, num_of_enc_layers, emb_size, batch_size, dropout, bi_direct):\n        super(GRU_Encoder, self).__init__()\n        self.input_size = input_size\n        self.hid_size = hid_size\n        self.num_of_enc_layers = num_of_enc_layers\n        self.emb_size = emb_size\n        self.batch_size = batch_size\n        self.bi_direct = bi_direct\n        self.dropout = dropout\n        self.embedding = nn.Embedding(input_size, emb_size)\n        # print(\"IS:{} ES:{}\".format(input_size, emb_size))\n        self.gru = nn.GRU(emb_size, hid_size, num_of_enc_layers, bidirectional = bi_direct, dropout = dropout)\n\n    def forward(self, input_data, hidden):\n        input_data = input_data.T\n        # print(input_data.shape)\n        embed = self.embedding(input_data).to(device_name)\n        output, hidden = self.gru(embed, hidden)\n        return output, hidden\n\n    def initialiseHidden(self):\n        if(self.bi_direct):\n            return torch.zeros(2*self.num_of_enc_layers, self.batch_size, self.hid_size, device = device_name)\n        else:\n            return torch.zeros(self.num_of_enc_layers, self.batch_size, self.hid_size, device = device_name)\n","metadata":{"id":"S4q529z-FjCx","execution":{"iopub.status.busy":"2023-05-22T13:13:22.470873Z","iopub.execute_input":"2023-05-22T13:13:22.471899Z","iopub.status.idle":"2023-05-22T13:13:22.484239Z","shell.execute_reply.started":"2023-05-22T13:13:22.47181Z","shell.execute_reply":"2023-05-22T13:13:22.482381Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class GRU_Decoder(nn.Module):\n    def __init__(self, op_size, num_of_dec_layers, hid_size, batch_size, emb_size, dropout, bi_direct):\n        super(GRU_Decoder, self).__init__()\n        self.op_size = op_size\n        self.hid_size = hid_size\n        self.num_of_dec_layers = num_of_dec_layers\n        self.emb_size = emb_size\n        self.batch_size = batch_size\n        self.bi_direct = bi_direct\n        self.embedding = nn.Embedding(op_size, emb_size)\n        self.op = nn.Linear(2*hid_size, op_size) if (bi_direct) else nn.Linear(hid_size, op_size)\n        self.softmax = nn.LogSoftmax(dim = 2)\n        self.gru = nn.GRU(emb_size, hid_size, num_of_dec_layers, bidirectional = bi_direct, dropout = dropout)\n\n    def forward(self, input_data, hidden):\n        # print(input_data)\n        embed = self.embedding(input_data)\n        embed = embed.view(-1, self.batch_size, self.emb_size)\n        #     print(hidden.shape)\n        out, hidden = self.gru(embed, hidden)\n        # print(out.shape)\n        temp = self.op(out)\n        out = self.softmax(temp)\n        return out, hidden","metadata":{"id":"uDr5Mjb6Fi0a","execution":{"iopub.status.busy":"2023-05-22T13:13:23.018709Z","iopub.execute_input":"2023-05-22T13:13:23.019693Z","iopub.status.idle":"2023-05-22T13:13:23.029624Z","shell.execute_reply.started":"2023-05-22T13:13:23.019648Z","shell.execute_reply":"2023-05-22T13:13:23.028405Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class RNN_Encoder(nn.Module):\n    def __init__(self, input_size, hid_size, num_of_enc_layers, emb_size, batch_size, dropout, bi_direct):\n        super(RNN_Encoder, self).__init__()\n        self.input_size = input_size\n        self.hid_size = hid_size\n        self.num_of_enc_layers = num_of_enc_layers\n        self.emb_size = emb_size\n        self.batch_size = batch_size\n        self.bi_direct = bi_direct\n        self.dropout = dropout\n        self.embedding = nn.Embedding(input_size, emb_size)\n        self.rnn = nn.RNN(emb_size, hid_size, num_of_enc_layers, bidirectional = bi_direct, dropout = dropout)\n\n    def forward(self, input_data, hidden):\n        input_data = input_data.T\n        embed = self.embedding(input_data).to(device_name)\n        output, hidden = self.rnn(embed, hidden)\n        return output, hidden\n\n    def initialiseHidden(self):\n        if(self.bi_direct):\n            return torch.zeros(2*self.num_of_enc_layers, self.batch_size, self.hid_size, device = device_name)\n        else:\n            return torch.zeros(self.num_of_enc_layers, self.batch_size, self.hid_size, device = device_name)\n  ","metadata":{"execution":{"iopub.status.busy":"2023-05-22T13:13:24.410942Z","iopub.execute_input":"2023-05-22T13:13:24.411696Z","iopub.status.idle":"2023-05-22T13:13:24.421725Z","shell.execute_reply.started":"2023-05-22T13:13:24.411651Z","shell.execute_reply":"2023-05-22T13:13:24.420471Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class RNN_Decoder(nn.Module):\n    def __init__(self, op_size, num_of_dec_layers, hid_size, batch_size, emb_size, dropout, bi_direct):\n        super(RNN_Decoder, self).__init__()\n        self.op_size = op_size\n        self.hid_size = hid_size\n        self.num_of_dec_layers = num_of_dec_layers\n        self.emb_size = emb_size\n        self.batch_size = batch_size\n        self.bi_direct = bi_direct\n        self.embedding = nn.Embedding(op_size, emb_size)\n        self.op = nn.Linear(2*hid_size, op_size) if (bi_direct) else nn.Linear(hid_size, op_size)\n        self.softmax = nn.LogSoftmax(dim = 2)\n        self.rnn = nn.RNN(emb_size, hid_size, num_of_dec_layers, bidirectional = bi_direct, dropout = dropout)\n\n    def forward(self, input_data, hidden):\n        embed = self.embedding(input_data)\n        embed = embed.view(-1, self.batch_size, self.emb_size)\n        out, hidden = self.rnn(embed, hidden)\n        temp = self.op(out)\n        out = self.softmax(temp)\n        return out, hidden","metadata":{"execution":{"iopub.status.busy":"2023-05-22T13:13:25.058678Z","iopub.execute_input":"2023-05-22T13:13:25.059785Z","iopub.status.idle":"2023-05-22T13:13:25.06888Z","shell.execute_reply.started":"2023-05-22T13:13:25.05974Z","shell.execute_reply":"2023-05-22T13:13:25.067853Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class LSTM_Encoder(nn.Module):\n    def __init__(self, input_size, hid_size, num_of_enc_layers, emb_size, batch_size, dropout, bi_direct):\n        super(LSTM_Encoder, self).__init__()\n        self.input_size = input_size\n        self.hid_size = hid_size\n        self.num_of_enc_layers = num_of_enc_layers\n        self.emb_size = emb_size\n        self.batch_size = batch_size\n        self.bi_direct = bi_direct\n        self.dropout = dropout\n        self.embedding = nn.Embedding(input_size, emb_size)\n        self.lstm = nn.LSTM(emb_size, hid_size, num_of_enc_layers, bidirectional = bi_direct, dropout = dropout)\n\n    def forward(self, input_data, hidden, state):\n        input_data = input_data.T\n        embed = self.embedding(input_data).to(device_name)\n        output, (hidden, state) = self.lstm(embed, (hidden, state))\n        return output, hidden, state\n\n    def initialiseHidden(self):\n        if(self.bi_direct):\n            return torch.zeros(2*self.num_of_enc_layers, self.batch_size, self.hid_size, device = device_name)\n        else:\n            return torch.zeros(self.num_of_enc_layers, self.batch_size, self.hid_size, device = device_name)\n  ","metadata":{"execution":{"iopub.status.busy":"2023-05-22T13:13:26.569894Z","iopub.execute_input":"2023-05-22T13:13:26.570866Z","iopub.status.idle":"2023-05-22T13:13:26.581958Z","shell.execute_reply.started":"2023-05-22T13:13:26.570792Z","shell.execute_reply":"2023-05-22T13:13:26.580684Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class LSTM_Decoder(nn.Module):\n    def __init__(self, op_size, num_of_dec_layers, hid_size, batch_size, emb_size, dropout, bi_direct):\n        super(LSTM_Decoder, self).__init__()\n        self.op_size = op_size\n        self.hid_size = hid_size\n        self.num_of_dec_layers = num_of_dec_layers\n        self.emb_size = emb_size\n        self.batch_size = batch_size\n        self.bi_direct = bi_direct\n        self.embedding = nn.Embedding(op_size, emb_size)\n        self.op = nn.Linear(2*hid_size, op_size) if (bi_direct) else nn.Linear(hid_size, op_size)\n        self.softmax = nn.LogSoftmax(dim = 2)\n        self.lstm = nn.LSTM(emb_size, hid_size, num_of_dec_layers, bidirectional = bi_direct, dropout = dropout)\n\n    def forward(self, input_data, hidden, state):\n        embed = self.embedding(input_data)\n        embed = embed.view(-1, self.batch_size, self.emb_size)\n        out, (hidden, state) = self.lstm(embed, (hidden, state))\n        temp = self.op(out)\n        out = self.softmax(temp)\n        return out, hidden, state","metadata":{"execution":{"iopub.status.busy":"2023-05-22T13:13:27.081753Z","iopub.execute_input":"2023-05-22T13:13:27.082654Z","iopub.status.idle":"2023-05-22T13:13:27.091897Z","shell.execute_reply.started":"2023-05-22T13:13:27.08261Z","shell.execute_reply":"2023-05-22T13:13:27.090877Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class Atten_decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, output_size, dec_layers, p, max_input_size, cell_type, bidirectional):\n        super(Atten_decoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.max_length =   max_input_size \n        self.dec_layers = dec_layers\n        self.dropout = nn.Dropout(p)\n        self.cell_type = cell_type\n        self.softmax = nn.LogSoftmax(dim = 0)\n        self.embedding = nn.Embedding(output_size, embedding_size)\n        if(cell_type == \"GRU\"):\n            self.gru = nn.GRU(hidden_size, hidden_size, dec_layers, dropout = p)\n        if(cell_type == \"RNN\"):\n            self.rnn = nn.RNN(hidden_size, hidden_size, dec_layers, dropout = p)\n        if(cell_type == \"LSTM\"):\n            self.lstm = nn.LSTM(hidden_size, hidden_size, dec_layers, dropout = p)\n        self.fc = nn.Linear(hidden_size, output_size)  # fully connected.\n        self.attn = nn.Linear(hidden_size+embedding_size, self.max_length)\n        if(bidirectional):\n            self.attn_combine = nn.Linear(hidden_size * 2 + embedding_size, hidden_size)\n        else :\n            self.attn_combine = nn.Linear(hidden_size + embedding_size, hidden_size)\n\n    def forward(self, x,output, hidden, cell = 0):\n        x = x.unsqueeze(0)\n        output=output.permute(1,0,2)\n#         print(\"X :\", x.shape)\n        embedded = self.embedding(x)\n        embedded = self.dropout(embedded)\n#         print(\"Emb-{},\\nHid-{}\".format(embedded.shape, hidden.shape))\n        attn_weights = self.softmax(self.attn(torch.cat((embedded[0],hidden[0]), 2)))\n        attn_applied = torch.bmm(attn_weights.unsqueeze(1),output)\n        attn_applied = attn_applied.squeeze(1)\n        op = torch.cat((embedded[0], attn_applied), 1)\n\n        op = self.attn_combine(op).unsqueeze(0)\n        op = nn.functional.relu(op)\n        if(self.cell_type == \"GRU\"):\n            outputs, hidden = self.gru(op, hidden)\n        if(self.cell_type == \"RNN\"):\n            outputs, hidden = self.rnn(op, hidden)\n        if(self.cell_type == \"LSTM\"):\n            outputs, (hidden, cell) = self.lstm(op, (hidden, cell))\n        predictions = self.fc(outputs)\n        # shape of predictions: (1, N, length_of_vocab)\n        predictions = predictions.squeeze(0)\n        # shape of predictions: (N, length_of_vocab)\n        if(self.cell_type == \"LSTM\"):\n            return predictions, hidden, attn_weights, attn_applied, cell\n        return predictions, hidden ,attn_weights, attn_applied","metadata":{"execution":{"iopub.status.busy":"2023-05-22T13:13:27.544626Z","iopub.execute_input":"2023-05-22T13:13:27.545036Z","iopub.status.idle":"2023-05-22T13:13:27.562567Z","shell.execute_reply.started":"2023-05-22T13:13:27.545002Z","shell.execute_reply":"2023-05-22T13:13:27.561634Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def trainWithoutAttention(input_data, target_data, loss_fn, enc_optimizer, dec_optimizer, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type):\n    teacher_forcing = 0.5\n    loss = 0\n    for b in range(0, len(input_data), batch_size):\n        x, y = input_data[b : b+batch_size], target_data[b : b+batch_size]\n        temp = 0\n        enc_optimizer.zero_grad()\n        dec_optimizer.zero_grad()\n        if(cell_type == 'GRU' or cell_type == 'RNN'):\n            enc_hidden = encoder.initialiseHidden()\n            enc_output, enc_hidden = encoder(x, enc_hidden)\n            dec_hidden = enc_hidden[-1].repeat(num_of_dec_layers, 1, 1)\n            if bi_direct:\n                dec_hidden = dec_hidden.repeat(2,1,1)\n            y = y.T\n            dec_input = y[0]\n            #       print(\"AFT_Decoder Hidden : {}\".format(dec_hidden.shape))\n            condition = False if random.random() > teacher_forcing else True\n            if(condition):\n                for i in range(len(y)):\n                    dec_output, dec_hidden = decoder(dec_input, dec_hidden)\n                    temp += loss_fn(torch.squeeze(dec_output), y[i])\n                    dec_input = y[i]\n            else:\n                for i in range(len(y)):\n                    dec_output, dec_hidden = decoder(dec_input, dec_hidden)\n                    prob, idx = dec_output.topk(1)\n                    temp += loss_fn(torch.squeeze(dec_output), y[i])\n                    dec_input = idx.squeeze().detach()\n                    \n        elif(cell_type == 'LSTM'):\n            enc_hidden = encoder.initialiseHidden()\n            enc_state = encoder.initialiseHidden()\n            \n            enc_output, enc_hidden, enc_state = encoder(x, enc_hidden, enc_state)\n            dec_hidden = enc_hidden[-1].repeat(num_of_dec_layers, 1, 1)\n            if bi_direct:\n                dec_hidden = dec_hidden.repeat(2,1,1)\n            \n            dec_state = enc_state[-1].repeat(num_of_dec_layers, 1, 1)\n            if bi_direct:\n                dec_state = dec_state.repeat(2,1,1)\n            y = y.T\n            dec_input = y[0]\n            #       print(\"AFT_Decoder Hidden : {}\".format(dec_hidden.shape))\n            condition = False if random.random() > teacher_forcing else True\n            if(condition):\n                for i in range(len(y)):\n                    dec_output, dec_hidden, dec_state = decoder(dec_input, dec_hidden, dec_state)\n                    temp += loss_fn(torch.squeeze(dec_output), y[i])\n                    dec_input = y[i]\n            else:\n                for i in range(len(y)):\n                    dec_output, dec_hidden, dec_state = decoder(dec_input, dec_hidden, dec_state)\n                    prob, idx = dec_output.topk(1)\n                    temp += loss_fn(torch.squeeze(dec_output), y[i])\n                    dec_input = idx.squeeze().detach()\n        \n        temp.backward()\n        enc_optimizer.step()\n        dec_optimizer.step()\n        loss += temp\n\n    return loss.item()/(len(target_data) * target_data.shape[1]), encoder, decoder\n\n","metadata":{"id":"KToV_mC2WtPI","execution":{"iopub.status.busy":"2023-05-22T13:13:28.249045Z","iopub.execute_input":"2023-05-22T13:13:28.249488Z","iopub.status.idle":"2023-05-22T13:13:28.270376Z","shell.execute_reply.started":"2023-05-22T13:13:28.249459Z","shell.execute_reply":"2023-05-22T13:13:28.269128Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def evalWithoutAttention(input_data, target_data, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type):\n    out = []\n    for b in range(0, len(input_data), batch_size):\n        x, y = input_data[b : b+batch_size], target_data[b : b+batch_size]\n        encoder.eval()\n        decoder.eval()\n        predicted_data = list()\n        if(cell_type == 'GRU' or cell_type == 'RNN'):\n            enc_hidden = encoder.initialiseHidden()\n            enc_output, enc_hidden = encoder(x, enc_hidden)\n            y = y.T      \n            dec_hidden = enc_hidden[-1].repeat(num_of_dec_layers, 1, 1)\n            #dec_hidden = enc_hidden\n            dec_input = y[0]\n            if bi_direct:\n                dec_hidden = dec_hidden.repeat(2,1,1)\n            for i in range(len(y)):\n                dec_output, dec_hidden = decoder(dec_input, dec_hidden)\n                prob, idx = dec_output.topk(1)\n                idx = idx.squeeze()\n                dec_input = idx\n                predicted_data.append(idx.tolist())\n            out.append(predicted_data)\n        elif(cell_type == 'LSTM'):\n            enc_hidden = encoder.initialiseHidden()\n            enc_state = encoder.initialiseHidden()\n            enc_output, enc_hidden, enc_state = encoder(x, enc_hidden, enc_state)\n            y = y.T      \n            dec_hidden = enc_hidden[-1].repeat(num_of_dec_layers, 1, 1)\n            if bi_direct:\n                dec_hidden = dec_hidden.repeat(2,1,1)\n                \n            dec_state = enc_state[-1].repeat(num_of_dec_layers, 1, 1)\n            if bi_direct:\n                dec_state = dec_state.repeat(2,1,1)\n            \n            dec_input = y[0]\n            for i in range(len(y)):\n                dec_output, dec_hidden, dec_state = decoder(dec_input, dec_hidden, dec_state)\n                prob, idx = dec_output.topk(1)\n                idx = idx.squeeze()\n                dec_input = idx\n                predicted_data.append(idx.tolist())\n            out.append(predicted_data)\n    return out","metadata":{"id":"tosOCKpmU49G","execution":{"iopub.status.busy":"2023-05-22T13:13:28.915538Z","iopub.execute_input":"2023-05-22T13:13:28.916764Z","iopub.status.idle":"2023-05-22T13:13:28.931039Z","shell.execute_reply.started":"2023-05-22T13:13:28.916712Z","shell.execute_reply":"2023-05-22T13:13:28.929975Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def trainWithAttention(input_data, target_data, loss_fn, enc_optimizer, dec_optimizer, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type):\n    teacher_forcing = 0.5\n    loss = 0\n    for b in range(0, len(input_data), batch_size):\n        x, y = input_data[b : b+batch_size], target_data[b : b+batch_size]\n        temp = 0\n        enc_optimizer.zero_grad()\n        dec_optimizer.zero_grad()\n        if(cell_type == 'GRU' or cell_type == 'RNN'):\n            enc_hidden = encoder.initialiseHidden()\n            enc_output, enc_hidden = encoder(x, enc_hidden)\n            dec_hidden = enc_hidden[-1].repeat(num_of_dec_layers, 1, 1)\n            if bi_direct:\n                dec_hidden = dec_hidden.repeat(2,1,1)\n            y = y.T\n            dec_input = y[0]\n            #       print(\"AFT_Decoder Hidden : {}\".format(dec_hidden.shape))\n            condition = False if random.random() > teacher_forcing else True\n            if(condition):\n                for i in range(len(y)):\n                    dec_output, dec_hidden , attn_weights, attn_applied= decoder(dec_input, enc_output, dec_hidden)\n                    temp += loss_fn(torch.squeeze(dec_output), y[i])\n                    dec_input = y[i]\n            else:\n                for i in range(len(y)):\n                    dec_output, dec_hidden , attn_weights, attn_applied= decoder(dec_input, enc_output, dec_hidden)\n                    prob, idx = dec_output.topk(1)\n                    temp += nn.NLLLoss()(nn.functional.log_softmax(dec_output, dim=1), y[i])\n#                     temp += loss_fn(torch.squeeze(dec_output), y[i])\n                    dec_input = idx.squeeze().detach()\n                    \n        elif(cell_type == 'LSTM'):\n            enc_hidden = encoder.initialiseHidden()\n            enc_state = encoder.initialiseHidden()\n            \n            enc_output, enc_hidden, enc_state = encoder(x, enc_hidden, enc_state)\n            dec_hidden = enc_hidden[-1].repeat(num_of_dec_layers, 1, 1)\n            if bi_direct:\n                dec_hidden = dec_hidden.repeat(2,1,1)\n            \n            dec_state = enc_state[-1].repeat(num_of_dec_layers, 1, 1)\n            if bi_direct:\n                dec_state = dec_state.repeat(2,1,1)\n            y = y.T\n            dec_input = y[0]\n            #       print(\"AFT_Decoder Hidden : {}\".format(dec_hidden.shape))\n            condition = False if random.random() > teacher_forcing else True\n            if(condition):\n                for i in range(len(y)):\n                    dec_output, dec_hidden, attn_weights, attn_applied, dec_state = decoder(dec_input, enc_output, dec_hidden, dec_state)\n                    temp += loss_fn(torch.squeeze(dec_output), y[i])\n                    dec_input = y[i]\n            else:\n                for i in range(len(y)):\n                    dec_output, dec_hidden, attn_weights, attn_applied, dec_state = decoder(dec_input, enc_output, dec_hidden, dec_state)\n                    prob, idx = dec_output.topk(1)\n#                     print(\"blhhh\")\n                    temp += nn.NLLLoss()(nn.functional.log_softmax(dec_output, dim=1), y[i])\n#                     temp += loss_fn(torch.squeeze(dec_output), y[i])\n                    dec_input = idx.squeeze().detach()\n        \n        temp.backward()\n        enc_optimizer.step()\n        dec_optimizer.step()\n        loss += temp\n\n    return loss.item()/(len(target_data) * target_data.shape[1]), encoder, decoder, attn_weights, attn_applied","metadata":{"execution":{"iopub.status.busy":"2023-05-22T13:13:29.778337Z","iopub.execute_input":"2023-05-22T13:13:29.779208Z","iopub.status.idle":"2023-05-22T13:13:29.797473Z","shell.execute_reply.started":"2023-05-22T13:13:29.779164Z","shell.execute_reply":"2023-05-22T13:13:29.796385Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def evalWithAttention(input_data, target_data, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type):\n    out = []\n    for b in range(0, len(input_data), batch_size):\n        x, y = input_data[b : b+batch_size], target_data[b : b+batch_size]\n        encoder.eval()\n        decoder.eval()\n        predicted_data = list()\n        if(cell_type == 'GRU' or cell_type == 'RNN'):\n            enc_hidden = encoder.initialiseHidden()\n            enc_output, enc_hidden = encoder(x, enc_hidden)\n            y = y.T      \n            dec_hidden = enc_hidden[-1].repeat(num_of_dec_layers, 1, 1)\n            #dec_hidden = enc_hidden\n            dec_input = y[0]\n            if bi_direct:\n                dec_hidden = dec_hidden.repeat(2,1,1)\n            for i in range(len(y)):\n#                 dec_output, dec_hidden = decoder(dec_input, dec_hidden)\n                dec_output, dec_hidden , attn_weights, attn_applied= decoder(dec_input, enc_output, dec_hidden)\n                prob, idx = dec_output.topk(1)\n                idx = idx.squeeze()\n                dec_input = idx\n                predicted_data.append(idx.tolist())\n            out.append(predicted_data)\n        elif(cell_type == 'LSTM'):\n            enc_hidden = encoder.initialiseHidden()\n            enc_state = encoder.initialiseHidden()\n            enc_output, enc_hidden, enc_state = encoder(x, enc_hidden, enc_state)\n            y = y.T      \n            dec_hidden = enc_hidden[-1].repeat(num_of_dec_layers, 1, 1)\n            if bi_direct:\n                dec_hidden = dec_hidden.repeat(2,1,1)\n                \n            dec_state = enc_state[-1].repeat(num_of_dec_layers, 1, 1)\n            if bi_direct:\n                dec_state = dec_state.repeat(2,1,1)\n            \n            dec_input = y[0]\n            for i in range(len(y)):\n                dec_output, dec_hidden, attn_weights, attn_applied, dec_state = decoder(dec_input, enc_output, dec_hidden, dec_state)\n                prob, idx = dec_output.topk(1)\n                idx = idx.squeeze()\n                dec_input = idx\n                predicted_data.append(idx.tolist())\n            out.append(predicted_data)\n    return out","metadata":{"execution":{"iopub.status.busy":"2023-05-22T13:13:31.583864Z","iopub.execute_input":"2023-05-22T13:13:31.584823Z","iopub.status.idle":"2023-05-22T13:13:31.598759Z","shell.execute_reply.started":"2023-05-22T13:13:31.584768Z","shell.execute_reply":"2023-05-22T13:13:31.597636Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def training(input_data, input_size, target_data, target_size, max_input_size, epochs, batch_size, emb_size, num_of_enc_layers, num_of_dec_layers, hid_size, cell_type, bi_direct, enc_dropout, dec_dropout, use_attention, beam_size):\n    learning_rate = 0.001\n    if(use_attention):\n        if(cell_type == \"GRU\"):\n            encoder = GRU_Encoder(input_size, hid_size, num_of_enc_layers, emb_size, batch_size, enc_dropout, bi_direct).to(device_name)\n        elif(cell_type == \"RNN\"):\n            encoder = RNN_Encoder(input_size, hid_size, num_of_enc_layers, emb_size, batch_size, enc_dropout, bi_direct).to(device_name)\n        elif(cell_type == \"LSTM\"):\n            encoder = LSTM_Encoder(input_size, hid_size, num_of_enc_layers, emb_size, batch_size, enc_dropout, bi_direct).to(device_name)\n        decoder = Atten_decoder(input_size, emb_size, hid_size, target_size, num_of_dec_layers, dec_dropout, max_input_size, cell_type, bi_direct).to(device_name)\n    else:\n        if(cell_type == \"GRU\"):\n            encoder = GRU_Encoder(input_size, hid_size, num_of_enc_layers, emb_size, batch_size, enc_dropout, bi_direct).to(device_name)\n            decoder = GRU_Decoder(target_size, num_of_dec_layers, hid_size, batch_size, emb_size, dec_dropout, bi_direct).to(device_name)\n        elif(cell_type == \"RNN\"):\n            encoder = RNN_Encoder(input_size, hid_size, num_of_enc_layers, emb_size, batch_size, enc_dropout, bi_direct).to(device_name)\n            decoder = RNN_Decoder(target_size, num_of_dec_layers, hid_size, batch_size, emb_size, dec_dropout, bi_direct).to(device_name)\n        elif(cell_type == \"LSTM\"):\n            encoder = LSTM_Encoder(input_size, hid_size, num_of_enc_layers, emb_size, batch_size, enc_dropout, bi_direct).to(device_name)\n            decoder = LSTM_Decoder(target_size, num_of_dec_layers, hid_size, batch_size, emb_size, dec_dropout, bi_direct).to(device_name)\n\n    enc_optimizer = torch.optim.Adam(encoder.parameters(), learning_rate)\n    dec_optimizer = torch.optim.Adam(decoder.parameters(), learning_rate)\n    loss_fn = nn.NLLLoss(reduction = 'sum')\n    encoder.train()\n    decoder.train()\n    train_loss = []\n    train_acc = []\n    val_acc = []\n    temp1 = 0\n    temp2 = 0\n    if(use_attention):\n        for i in range(epochs):\n            encoder.train()\n            decoder.train()\n            loss, encoder, decoder ,attn_weights, attn_applied = trainWithAttention(input_data, target_data, loss_fn, enc_optimizer, dec_optimizer, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type)\n            train_loss.append(loss)\n            trained_pred = evalWithAttention(tensor_eng, tensor_hin, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type)\n            acc = calculateAccuracy(trained_pred, tensor_hin)\n            train_acc.append(acc)\n            trained_pred_val = evalWithAttention(tensor_eng_val, tensor_hin_val, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type)\n            vacc = calculateAccuracy(trained_pred_val, tensor_hin_val)\n            val_acc.append(vacc)\n            print(\"Epoch : {} \\tLoss : {}\".format(i, loss))\n        \n        temp1 = attn_weights\n        temp2 = attn_applied\n    else:\n        for i in range(epochs):\n            encoder.train()\n            decoder.train()\n            loss, encoder, decoder = trainWithoutAttention(input_data, target_data, loss_fn, enc_optimizer, dec_optimizer, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type)\n            train_loss.append(loss)\n            trained_pred = evalWithoutAttention(tensor_eng, tensor_hin, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type)\n            acc = calculateAccuracy(trained_pred, tensor_hin)\n            train_acc.append(acc)\n            trained_pred_val = evalWithoutAttention(tensor_eng_val, tensor_hin_val, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type)\n            vacc = calculateAccuracy(trained_pred_val, tensor_hin_val)\n            val_acc.append(vacc)\n            print(\"Epoch : {} \\tLoss : {}\".format(i, loss))\n    \n    return encoder, decoder, train_loss, train_acc, val_acc, temp1, temp2\n","metadata":{"id":"ifkAGoezFiwH","execution":{"iopub.status.busy":"2023-05-22T13:13:32.211061Z","iopub.execute_input":"2023-05-22T13:13:32.211445Z","iopub.status.idle":"2023-05-22T13:13:32.231342Z","shell.execute_reply.started":"2023-05-22T13:13:32.211413Z","shell.execute_reply":"2023-05-22T13:13:32.230344Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#Loading the dataset and preprocessing\n\ndf = pd.read_csv('/kaggle/input/fdl-a3/hin_train.csv', names=['eng','hin'])\ndf_test = pd.read_csv('/kaggle/input/fdl-a3/hin_test.csv', names=['eng','hin'])\ndf_valid = pd.read_csv('/kaggle/input/fdl-a3/hin_valid.csv', names=['eng','hin'])\neng_maxlen = len(max(df['eng'], key=len))\nhin_maxlen = len(max(df['hin'], key=len))\nmax_len = max(eng_maxlen, hin_maxlen)\neng_words = df['eng'].copy()\nhin_words = df['hin'].copy()\n\nunique_eng_letters = set(''.join(eng_words))\nunique_eng_letters.add('*')\n\n\nunique_hin_letters = set(''.join(hin_words))\nunique_hin_letters.add('#')\nunique_hin_letters.add('*')\n\nint_to_eng = dict(enumerate(unique_eng_letters))\neng_to_int = {char: ind for ind, char in int_to_eng.items()}\n\nint_to_hin = dict(enumerate(unique_hin_letters))\nhin_to_int = {char: ind for ind, char in int_to_hin.items()}\nhin_to_int['_'] = len(hin_to_int)\n\ntensor_eng, tensor_hin = preprocessingData(df, max_len, eng_to_int, hin_to_int)\ntensor_eng_test, tensor_hin_test = preprocessingData(df_test, max_len, eng_to_int, hin_to_int)\ntensor_eng_val, tensor_hin_val = preprocessingData(df_valid, max_len, eng_to_int, hin_to_int)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T13:13:32.920091Z","iopub.execute_input":"2023-05-22T13:13:32.921056Z","iopub.status.idle":"2023-05-22T13:13:40.505734Z","shell.execute_reply.started":"2023-05-22T13:13:32.921007Z","shell.execute_reply":"2023-05-22T13:13:40.504686Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#Configuration of the model\n\ninput_data = tensor_eng\ninput_size = len(unique_eng_letters)\ntarget_data = tensor_hin\ntarget_size = len(unique_hin_letters) \nmax_input_size = tensor_eng.shape[1] \nepochs = 10\nbatch_size = 256 \nemb_size = 512 \nnum_of_enc_layers = 3\nnum_of_dec_layers = 3\nhid_size = 512\ncell_type = \"LSTM\" \nbi_direct = True \nenc_dropout = 0.3\ndec_dropout = 0.1\nbeam_size = 1\nuse_attention = False","metadata":{"execution":{"iopub.status.busy":"2023-05-22T13:13:40.507691Z","iopub.execute_input":"2023-05-22T13:13:40.508155Z","iopub.status.idle":"2023-05-22T13:13:40.51499Z","shell.execute_reply.started":"2023-05-22T13:13:40.50812Z","shell.execute_reply":"2023-05-22T13:13:40.513886Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#Block of code to train the model\n\nencoder, decoder, train_loss, train_acc, val_acc, temp1, temp2 = training(input_data, input_size, target_data, target_size, max_input_size, epochs, batch_size, emb_size, num_of_enc_layers, num_of_dec_layers, hid_size, cell_type, bi_direct, enc_dropout, dec_dropout, use_attention, beam_size)\nif(use_attention):\n    trained_pred = evalWithAttention(tensor_eng, tensor_hin, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type)\nelse:\n    trained_pred = evalWithoutAttention(tensor_eng, tensor_hin, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type)\ncalculateAccuracy(trained_pred, tensor_hin)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T13:13:46.290713Z","iopub.execute_input":"2023-05-22T13:13:46.291157Z","iopub.status.idle":"2023-05-22T13:27:39.533431Z","shell.execute_reply.started":"2023-05-22T13:13:46.291122Z","shell.execute_reply":"2023-05-22T13:27:39.532198Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Epoch : 0 \tLoss : 0.9295275426793982\nEpoch : 1 \tLoss : 0.39543628833912037\nEpoch : 2 \tLoss : 0.31561272515190975\nEpoch : 3 \tLoss : 0.25897040473090277\nEpoch : 4 \tLoss : 0.22920066550925927\nEpoch : 5 \tLoss : 0.21472488968460648\nEpoch : 6 \tLoss : 0.19993019386574074\nEpoch : 7 \tLoss : 0.18572965268735533\nEpoch : 8 \tLoss : 0.1757969495985243\nEpoch : 9 \tLoss : 0.1607646009657118\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"0.58232421875"},"metadata":{}}]},{"cell_type":"code","source":"#BLock of code to see the test accuracy\n\nif(use_attention):\n    trained_pred = evalWithAttention(tensor_eng_test, tensor_hin_test, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type)\nelse:\n    trained_pred = evalWithoutAttention(tensor_eng_test, tensor_hin_test, encoder, decoder, num_of_enc_layers, num_of_dec_layers, batch_size, bi_direct, cell_type)\ncalculateAccuracy(trained_pred, tensor_hin_test)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T13:34:53.864944Z","iopub.execute_input":"2023-05-22T13:34:53.865727Z","iopub.status.idle":"2023-05-22T13:34:55.62694Z","shell.execute_reply.started":"2023-05-22T13:34:53.865689Z","shell.execute_reply":"2023-05-22T13:34:55.625964Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"0.3408203125"},"metadata":{}}]},{"cell_type":"code","source":"#Block of code to generate csv file\n\nhin_words = []\nfor temp in trained_pred:\n    temp = torch.tensor(temp).T\n    for i in temp:\n        s=''\n        for j in range(1,len(i)):\n            if(int_to_hin[i[j].item()] == '*'):\n                hin_words.append(s)\n                break\n            s += int_to_hin[i[j].item()]\n\nres = df_test.copy()\nres['predicted'] = hin_words\nres.to_csv('predicted_hin_words.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SWEEP CONFIG","metadata":{}},{"cell_type":"code","source":"wandb.login(key = '5b3ff6cba361172038b8948f6dace9286a5bbfa0')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sweep_configuration = {\n    'method': 'bayes',\n    'name': 'fdl_a31',\n     'metric': {\n        'goal': 'maximize', \n        'name': 'validation_accuracy'\n        },\n    'parameters': {\n        'batch_size': {'values': [16, 32, 64, 256, 512]},\n        'epochs': {'values': [10]},\n        'num_of_enc_layers':{'values' : [1, 2, 3, 4]},\n        'num_of_dec_layers':{'values' : [1, 2, 3, 4]},\n        'hid_size': {'values' : [16, 32, 64, 256, 512]},\n        'emb_size': {'values' : [16, 32, 64, 256, 512]},\n        'cell_type':{'values' : ['GRU', 'LSTM', 'RNN']},\n        'bi_direct':{'values' : [True, False]},\n        'use_attention':{'values' : [False]},\n        'enc_dropout': {'values' : [0.1, 0.2, 0.3]},\n        'dec_dropout': {'values' : [0.1, 0.2, 0.3]},\n        'beam_size': {'values' : [1]}\n     }\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sweepTrain():\n    wandb.init()\n\n    epochs = wandb.config.epochs\n    batch_size = wandb.config.batch_size \n    emb_size = wandb.config.emb_size\n    num_of_enc_layers = wandb.config.num_of_enc_layers\n    num_of_dec_layers = wandb.config.num_of_dec_layers\n    hid_size = wandb.config.hid_size\n    cell_type = wandb.config.cell_type\n    bi_direct = wandb.config.bi_direct\n    enc_dropout = wandb.config.enc_dropout\n    dec_dropout = wandb.config.dec_dropout\n    beam_size = wandb.config.batch_size\n    use_attention = wandb.config.use_attention\n    \n#     wandb.run.name = f'ct_{cell_type}_hid_{hid_size}_emb_{emb_size}_ep_{epochs}_batch_{batch_size}_noel_{num_of_enc_layers}_nodl_{num_of_dec_layers}_bd_{bi_direct}_enc_dp_{enc_dropout}_dec_dp_{enc_dropout}_bs_{beam_size}'\n    wandb.run.name = f'inp_emb_{emb_size}_encl_{num_of_enc_layers}_decl_{num_of_dec_layers}_hid_{hid_size}_cel_{cell_type}_dp_{enc_dropout}'\n    encoder, decoder, train_loss, train_acc, val_acc, temp1, temp2 = training(input_data, input_size, target_data, target_size, max_input_size, epochs, batch_size, emb_size, num_of_enc_layers, num_of_dec_layers, hid_size, cell_type, bi_direct, enc_dropout, dec_dropout, use_attention, beam_size)\n    for i in range(len(train_loss)):\n        wandb.log({'training_loss': train_loss[i],\n                  'training_accuracy': train_acc[i],\n                  'validation_accuracy': val_acc[i]\n                  })","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep=sweep_configuration, project='fdl_a32')\nwandb.agent(sweep_id, function=sweepTrain)\nwandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}